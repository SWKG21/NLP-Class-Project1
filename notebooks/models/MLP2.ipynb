{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import  MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>cn</th>\n",
       "      <th>aai</th>\n",
       "      <th>year_diff</th>\n",
       "      <th>common_authors</th>\n",
       "      <th>title_overlap</th>\n",
       "      <th>journal_overlap</th>\n",
       "      <th>wmd</th>\n",
       "      <th>lr_stack</th>\n",
       "      <th>rf_stack</th>\n",
       "      <th>svc_stack</th>\n",
       "      <th>xgb_stack</th>\n",
       "      <th>res_allo_ind</th>\n",
       "      <th>tgt_citation</th>\n",
       "      <th>jaccard_nx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064373</td>\n",
       "      <td>1</td>\n",
       "      <td>0.513898</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.872725</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>8</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021211</td>\n",
       "      <td>20</td>\n",
       "      <td>4.320366</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.335541</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226401</td>\n",
       "      <td>124</td>\n",
       "      <td>0.097087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017202</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.087327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012634</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.602484</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.059588</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.414463</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sim  cn       aai  year_diff  common_authors  title_overlap  \\\n",
       "0  0.064373   1  0.513898          0             0.0       0.285714   \n",
       "1  0.021211  20  4.320366          1             0.0       0.250000   \n",
       "2  0.017202   0  0.000000         -2             0.0       0.000000   \n",
       "3  0.012634   0  0.000000         -4             0.0       0.000000   \n",
       "4  0.059588   0  0.000000         -5             0.0       0.000000   \n",
       "\n",
       "   journal_overlap       wmd  lr_stack  rf_stack  svc_stack  xgb_stack  \\\n",
       "0              1.0  1.872725         1         1          1          1   \n",
       "1              0.0  1.335541         1         1          1          1   \n",
       "2              0.0  2.087327         0         0          0          0   \n",
       "3              0.0  1.602484         0         0          0          0   \n",
       "4              0.0  1.414463         0         0          0          0   \n",
       "\n",
       "   res_allo_ind  tgt_citation  jaccard_nx  \n",
       "0      0.142857             8    0.058824  \n",
       "1      0.226401           124    0.097087  \n",
       "2      0.000000             2    0.000000  \n",
       "3      0.000000             2    0.000000  \n",
       "4      0.000000             2    0.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1 = pd.read_csv('../data/train_stacked.csv')\n",
    "train2 = pd.read_csv('../data/train_nx_ig_v2.csv')\n",
    "X_train = pd.concat([train1.loc[:,['sim','cn','aai','year_diff','common_authors','title_overlap','journal_overlap','wmd','lr_stack','rf_stack','svc_stack','xgb_stack']], train2.loc[:,['res_allo_ind','tgt_citation','jaccard_nx']]], axis=1)\n",
    "y_train = train1.loc[:,'link']\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>cn</th>\n",
       "      <th>aai</th>\n",
       "      <th>year_diff</th>\n",
       "      <th>common_authors</th>\n",
       "      <th>title_overlap</th>\n",
       "      <th>journal_overlap</th>\n",
       "      <th>wmd</th>\n",
       "      <th>lr_stack</th>\n",
       "      <th>rf_stack</th>\n",
       "      <th>svc_stack</th>\n",
       "      <th>xgb_stack</th>\n",
       "      <th>res_allo_ind</th>\n",
       "      <th>tgt_citation</th>\n",
       "      <th>jaccard_nx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.071870</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.808661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.163040</td>\n",
       "      <td>24</td>\n",
       "      <td>5.377973</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.515163</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311535</td>\n",
       "      <td>39</td>\n",
       "      <td>0.074303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.138004</td>\n",
       "      <td>59</td>\n",
       "      <td>15.053612</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.822632</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.342594</td>\n",
       "      <td>726</td>\n",
       "      <td>0.065338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.101857</td>\n",
       "      <td>21</td>\n",
       "      <td>4.899424</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.841217</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.298419</td>\n",
       "      <td>16</td>\n",
       "      <td>0.221053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.091231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.462258</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sim  cn        aai  year_diff  common_authors  title_overlap  \\\n",
       "0  0.071870   0   0.000000          0             0.0       0.000000   \n",
       "1  0.163040  24   5.377973          1             0.0       0.444444   \n",
       "2  0.138004  59  15.053612          2             0.0       0.285714   \n",
       "3  0.101857  21   4.899424          0             0.0       0.153846   \n",
       "4  0.091231   0   0.000000          5             0.0       0.000000   \n",
       "\n",
       "   journal_overlap       wmd  lr_stack  rf_stack  svc_stack  xgb_stack  \\\n",
       "0              0.5  1.808661         0         0          0          0   \n",
       "1              0.0  1.515163         1         1          1          1   \n",
       "2              1.0  1.822632         1         1          1          1   \n",
       "3              1.0  1.841217         1         1          1          1   \n",
       "4              0.0  1.462258         1         0          0          1   \n",
       "\n",
       "   res_allo_ind  tgt_citation  jaccard_nx  \n",
       "0      0.000000             3    0.000000  \n",
       "1      0.311535            39    0.074303  \n",
       "2      1.342594           726    0.065338  \n",
       "3      0.298419            16    0.221053  \n",
       "4      0.000000           144    0.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = pd.read_csv('../data/test_stacked.csv')\n",
    "test2 = pd.read_csv('../data/test_nx_ig_v2.csv')\n",
    "X_test = pd.concat([test1.loc[:,['sim','cn','aai','year_diff','common_authors','title_overlap','journal_overlap','wmd','lr_stack','rf_stack','svc_stack','xgb_stack']], test2.loc[:,['res_allo_ind','tgt_citation','jaccard_nx']]], axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.11911468\n",
      "Iteration 1, loss = 0.11956419\n",
      "Iteration 1, loss = 0.21409976\n",
      "Iteration 1, loss = 0.11986363\n",
      "Iteration 2, loss = 0.08955176\n",
      "Iteration 2, loss = 0.09042254\n",
      "Iteration 2, loss = 0.11133976\n",
      "Iteration 2, loss = 0.09083481\n",
      "Iteration 3, loss = 0.08729900\n",
      "Iteration 3, loss = 0.08800738\n",
      "Iteration 3, loss = 0.09812558\n",
      "Iteration 3, loss = 0.08838659\n",
      "Iteration 4, loss = 0.08696148\n",
      "Iteration 4, loss = 0.08631025\n",
      "Iteration 4, loss = 0.08715707\n",
      "Iteration 4, loss = 0.09303694\n",
      "Iteration 5, loss = 0.08632391\n",
      "Iteration 5, loss = 0.08580091\n",
      "Iteration 5, loss = 0.08627023\n",
      "Iteration 5, loss = 0.08928903\n",
      "Iteration 6, loss = 0.08578267\n",
      "Iteration 6, loss = 0.08528954\n",
      "Iteration 6, loss = 0.08572762\n",
      "Iteration 6, loss = 0.08700887\n",
      "Iteration 7, loss = 0.08536675\n",
      "Iteration 7, loss = 0.08496779\n",
      "Iteration 7, loss = 0.08514570\n",
      "Iteration 7, loss = 0.08578375\n",
      "Iteration 8, loss = 0.08506980\n",
      "Iteration 8, loss = 0.08477009\n",
      "Iteration 8, loss = 0.08469727\n",
      "Iteration 8, loss = 0.08502426\n",
      "Iteration 9, loss = 0.08466930\n",
      "Iteration 9, loss = 0.08459599\n",
      "Iteration 9, loss = 0.08447680\n",
      "Iteration 9, loss = 0.08466834\n",
      "Iteration 10, loss = 0.08451372\n",
      "Iteration 10, loss = 0.08436418\n",
      "Iteration 10, loss = 0.08425737\n",
      "Iteration 10, loss = 0.08434220\n",
      "Iteration 11, loss = 0.08430514\n",
      "Iteration 11, loss = 0.08412547\n",
      "Iteration 11, loss = 0.08426011\n",
      "Iteration 11, loss = 0.08402576\n",
      "Iteration 12, loss = 0.08405984\n",
      "Iteration 12, loss = 0.08403301\n",
      "Iteration 12, loss = 0.08397289\n",
      "Iteration 12, loss = 0.08378767\n",
      "Iteration 13, loss = 0.08388793\n",
      "Iteration 13, loss = 0.08362274\n",
      "Iteration 13, loss = 0.08385020\n",
      "Iteration 13, loss = 0.08387981\n",
      "Iteration 14, loss = 0.08389731\n",
      "Iteration 14, loss = 0.08384193\n",
      "Iteration 14, loss = 0.08349395\n",
      "Iteration 14, loss = 0.08369562\n",
      "Iteration 15, loss = 0.08363571\n",
      "Iteration 15, loss = 0.08358653\n",
      "Iteration 15, loss = 0.08338721\n",
      "Iteration 15, loss = 0.08357472\n",
      "Iteration 16, loss = 0.08353353\n",
      "Iteration 16, loss = 0.08348166\n",
      "Iteration 16, loss = 0.08329150\n",
      "Iteration 16, loss = 0.08347246\n",
      "Iteration 17, loss = 0.08356316\n",
      "Iteration 17, loss = 0.08331884\n",
      "Iteration 17, loss = 0.08330308\n",
      "Iteration 17, loss = 0.08326255\n",
      "Iteration 18, loss = 0.08342289\n",
      "Iteration 18, loss = 0.08332230\n",
      "Iteration 18, loss = 0.08316610\n",
      "Iteration 18, loss = 0.08314116\n",
      "Iteration 19, loss = 0.08326550\n",
      "Iteration 19, loss = 0.08324969\n",
      "Iteration 19, loss = 0.08324952\n",
      "Iteration 19, loss = 0.08305786\n",
      "Iteration 20, loss = 0.08323209\n",
      "Iteration 20, loss = 0.08311062\n",
      "Iteration 20, loss = 0.08303471\n",
      "Iteration 20, loss = 0.08303084\n",
      "Iteration 21, loss = 0.08306759\n",
      "Iteration 21, loss = 0.08311903\n",
      "Iteration 21, loss = 0.08297077\n",
      "Iteration 21, loss = 0.08290167\n",
      "Iteration 22, loss = 0.08304217\n",
      "Iteration 22, loss = 0.08300902\n",
      "Iteration 22, loss = 0.08293040\n",
      "Iteration 22, loss = 0.08282623\n",
      "Iteration 23, loss = 0.08294763\n",
      "Iteration 23, loss = 0.08286225\n",
      "Iteration 23, loss = 0.08273926\n",
      "Iteration 23, loss = 0.08284173\n",
      "Iteration 24, loss = 0.08287423\n",
      "Iteration 24, loss = 0.08270881\n",
      "Iteration 24, loss = 0.08273521\n",
      "Iteration 24, loss = 0.08264290\n",
      "Iteration 25, loss = 0.08280284\n",
      "Iteration 25, loss = 0.08269280\n",
      "Iteration 25, loss = 0.08256414\n",
      "Iteration 25, loss = 0.08255555\n",
      "Iteration 26, loss = 0.08255590\n",
      "Iteration 26, loss = 0.08260211\n",
      "Iteration 26, loss = 0.08261072\n",
      "Iteration 26, loss = 0.08239678\n",
      "Iteration 27, loss = 0.08261941\n",
      "Iteration 27, loss = 0.08258486\n",
      "Iteration 27, loss = 0.08260615\n",
      "Iteration 27, loss = 0.08250710\n",
      "Iteration 28, loss = 0.08260790\n",
      "Iteration 28, loss = 0.08248302\n",
      "Iteration 28, loss = 0.08245035\n",
      "Iteration 28, loss = 0.08241962\n",
      "Iteration 29, loss = 0.08254598\n",
      "Iteration 29, loss = 0.08242221\n",
      "Iteration 29, loss = 0.08242584\n",
      "Iteration 29, loss = 0.08231429\n",
      "Iteration 30, loss = 0.08244295\n",
      "Iteration 30, loss = 0.08239696\n",
      "Iteration 30, loss = 0.08237014\n",
      "Iteration 30, loss = 0.08234527\n",
      "Iteration 31, loss = 0.08243951\n",
      "Iteration 31, loss = 0.08225839\n",
      "Iteration 31, loss = 0.08225137\n",
      "Iteration 31, loss = 0.08227049\n",
      "Iteration 32, loss = 0.08237702\n",
      "Iteration 32, loss = 0.08220579\n",
      "Iteration 32, loss = 0.08220759\n",
      "Iteration 32, loss = 0.08229700\n",
      "Iteration 33, loss = 0.08242884\n",
      "Iteration 33, loss = 0.08209704\n",
      "Iteration 33, loss = 0.08216452\n",
      "Iteration 33, loss = 0.08229257\n",
      "Iteration 34, loss = 0.08228123\n",
      "Iteration 34, loss = 0.08204205\n",
      "Iteration 34, loss = 0.08213281\n",
      "Iteration 34, loss = 0.08215216\n",
      "Iteration 35, loss = 0.08217040\n",
      "Iteration 35, loss = 0.08199364\n",
      "Iteration 35, loss = 0.08203000\n",
      "Iteration 35, loss = 0.08205762\n",
      "Iteration 36, loss = 0.08211540\n",
      "Iteration 36, loss = 0.08199381\n",
      "Iteration 36, loss = 0.08204527\n",
      "Iteration 36, loss = 0.08201677\n",
      "Iteration 37, loss = 0.08212272\n",
      "Iteration 37, loss = 0.08191070\n",
      "Iteration 37, loss = 0.08191748\n",
      "Iteration 37, loss = 0.08198410\n",
      "Iteration 38, loss = 0.08192699\n",
      "Iteration 38, loss = 0.08187909\n",
      "Iteration 38, loss = 0.08190771\n",
      "Iteration 38, loss = 0.08199335\n",
      "Iteration 39, loss = 0.08180295\n",
      "Iteration 39, loss = 0.08170603\n",
      "Iteration 39, loss = 0.08181591\n",
      "Iteration 39, loss = 0.08193533\n",
      "Iteration 40, loss = 0.08180485\n",
      "Iteration 40, loss = 0.08165839\n",
      "Iteration 40, loss = 0.08171794\n",
      "Iteration 40, loss = 0.08191340\n",
      "Iteration 41, loss = 0.08174360\n",
      "Iteration 41, loss = 0.08157956\n",
      "Iteration 41, loss = 0.08188652\n",
      "Iteration 41, loss = 0.08164959\n",
      "Iteration 42, loss = 0.08158145\n",
      "Iteration 42, loss = 0.08161100\n",
      "Iteration 42, loss = 0.08168425\n",
      "Iteration 42, loss = 0.08185448\n",
      "Iteration 43, loss = 0.08164300\n",
      "Iteration 43, loss = 0.08154931\n",
      "Iteration 43, loss = 0.08159982\n",
      "Iteration 43, loss = 0.08179917\n",
      "Iteration 44, loss = 0.08148863\n",
      "Iteration 44, loss = 0.08153677\n",
      "Iteration 44, loss = 0.08161838\n",
      "Iteration 44, loss = 0.08174772\n",
      "Iteration 45, loss = 0.08156878\n",
      "Iteration 45, loss = 0.08155440\n",
      "Iteration 45, loss = 0.08153255\n",
      "Iteration 45, loss = 0.08173479\n",
      "Iteration 46, loss = 0.08131782\n",
      "Iteration 46, loss = 0.08157984\n",
      "Iteration 46, loss = 0.08139452\n",
      "Iteration 46, loss = 0.08168653\n",
      "Iteration 47, loss = 0.08142922\n",
      "Iteration 47, loss = 0.08150489\n",
      "Iteration 47, loss = 0.08139732\n",
      "Iteration 47, loss = 0.08173577\n",
      "Iteration 48, loss = 0.08132972\n",
      "Iteration 48, loss = 0.08139693\n",
      "Iteration 48, loss = 0.08129936\n",
      "Iteration 48, loss = 0.08171031\n",
      "Iteration 49, loss = 0.08136845\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 0.08143172\n",
      "Iteration 49, loss = 0.08125075\n",
      "Iteration 49, loss = 0.08157411\n",
      "Iteration 50, loss = 0.08134294\n",
      "Iteration 1, loss = 0.12094676\n",
      "Iteration 50, loss = 0.08121657\n",
      "Iteration 50, loss = 0.08159176\n",
      "Iteration 51, loss = 0.08137189\n",
      "Iteration 2, loss = 0.09342180\n",
      "Iteration 51, loss = 0.08129091\n",
      "Iteration 51, loss = 0.08153929\n",
      "Iteration 52, loss = 0.08132583\n",
      "Iteration 3, loss = 0.08984802\n",
      "Iteration 52, loss = 0.08121194\n",
      "Iteration 52, loss = 0.08159200\n",
      "Iteration 53, loss = 0.08125684\n",
      "Iteration 53, loss = 0.08154018\n",
      "Iteration 53, loss = 0.08115481\n",
      "Iteration 4, loss = 0.08784357\n",
      "Iteration 54, loss = 0.08123873\n",
      "Iteration 54, loss = 0.08148013\n",
      "Iteration 54, loss = 0.08117154\n",
      "Iteration 5, loss = 0.08638109\n",
      "Iteration 55, loss = 0.08120272\n",
      "Iteration 55, loss = 0.08156370\n",
      "Iteration 55, loss = 0.08112164\n",
      "Iteration 6, loss = 0.08560821\n",
      "Iteration 56, loss = 0.08119186\n",
      "Iteration 56, loss = 0.08140444\n",
      "Iteration 56, loss = 0.08103076\n",
      "Iteration 7, loss = 0.08499137\n",
      "Iteration 57, loss = 0.08113287\n",
      "Iteration 57, loss = 0.08141952\n",
      "Iteration 8, loss = 0.08465856\n",
      "Iteration 57, loss = 0.08099403\n",
      "Iteration 58, loss = 0.08110441\n",
      "Iteration 58, loss = 0.08143316\n",
      "Iteration 9, loss = 0.08429274\n",
      "Iteration 58, loss = 0.08092658\n",
      "Iteration 59, loss = 0.08110787\n",
      "Iteration 59, loss = 0.08132493\n",
      "Iteration 10, loss = 0.08406973\n",
      "Iteration 59, loss = 0.08090498\n",
      "Iteration 60, loss = 0.08104435\n",
      "Iteration 60, loss = 0.08130164\n",
      "Iteration 11, loss = 0.08383837\n",
      "Iteration 60, loss = 0.08090396\n",
      "Iteration 61, loss = 0.08108255\n",
      "Iteration 61, loss = 0.08125356\n",
      "Iteration 12, loss = 0.08368474\n",
      "Iteration 61, loss = 0.08086274\n",
      "Iteration 62, loss = 0.08104733\n",
      "Iteration 62, loss = 0.08123280\n",
      "Iteration 13, loss = 0.08364893\n",
      "Iteration 62, loss = 0.08079474\n",
      "Iteration 63, loss = 0.08119250\n",
      "Iteration 63, loss = 0.08107479\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.08357449\n",
      "Iteration 63, loss = 0.08095764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64, loss = 0.08123202\n",
      "Iteration 15, loss = 0.08339334\n",
      "Iteration 64, loss = 0.08079891\n",
      "Iteration 65, loss = 0.08109654\n",
      "Iteration 16, loss = 0.08327266\n",
      "Iteration 65, loss = 0.08076835\n",
      "Iteration 66, loss = 0.08112393\n",
      "Iteration 17, loss = 0.08325708\n",
      "Iteration 66, loss = 0.08067759\n",
      "Iteration 67, loss = 0.08115194\n",
      "Iteration 18, loss = 0.08318670\n",
      "Iteration 67, loss = 0.08069691\n",
      "Iteration 19, loss = 0.08302091\n",
      "Iteration 68, loss = 0.08109546\n",
      "Iteration 68, loss = 0.08062206\n",
      "Iteration 20, loss = 0.08295613\n",
      "Iteration 69, loss = 0.08105089\n",
      "Iteration 69, loss = 0.08071815\n",
      "Iteration 21, loss = 0.08306687\n",
      "Iteration 70, loss = 0.08103094\n",
      "Iteration 70, loss = 0.08055823\n",
      "Iteration 22, loss = 0.08283530\n",
      "Iteration 71, loss = 0.08098337\n",
      "Iteration 71, loss = 0.08066277\n",
      "Iteration 23, loss = 0.08280017\n",
      "Iteration 24, loss = 0.08281588\n",
      "Iteration 72, loss = 0.08100610\n",
      "Iteration 72, loss = 0.08058900\n",
      "Iteration 25, loss = 0.08276804\n",
      "Iteration 73, loss = 0.08091670\n",
      "Iteration 73, loss = 0.08053324\n",
      "Iteration 26, loss = 0.08264464\n",
      "Iteration 74, loss = 0.08048290\n",
      "Iteration 74, loss = 0.08087928\n",
      "Iteration 27, loss = 0.08257873\n",
      "Iteration 28, loss = 0.08254475\n",
      "Iteration 75, loss = 0.08045576\n",
      "Iteration 75, loss = 0.08079888\n",
      "Iteration 29, loss = 0.08253458\n",
      "Iteration 76, loss = 0.08092230\n",
      "Iteration 76, loss = 0.08036473\n",
      "Iteration 30, loss = 0.08249092\n",
      "Iteration 77, loss = 0.08080000\n",
      "Iteration 77, loss = 0.08032419\n",
      "Iteration 31, loss = 0.08236928\n",
      "Iteration 78, loss = 0.08037158\n",
      "Iteration 78, loss = 0.08069665\n",
      "Iteration 79, loss = 0.08036339\n",
      "Iteration 32, loss = 0.08233387\n",
      "Iteration 79, loss = 0.08068561\n",
      "Iteration 80, loss = 0.08033007\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.08218408\n",
      "Iteration 80, loss = 0.08074275\n",
      "Iteration 34, loss = 0.08218062\n",
      "Iteration 81, loss = 0.08071340\n",
      "Iteration 35, loss = 0.08215184\n",
      "Iteration 82, loss = 0.08064398\n",
      "Iteration 36, loss = 0.08194353\n",
      "Iteration 83, loss = 0.08070629\n",
      "Iteration 37, loss = 0.08185716\n",
      "Iteration 84, loss = 0.08064059\n",
      "Iteration 38, loss = 0.08182629\n",
      "Iteration 85, loss = 0.08055665\n",
      "Iteration 39, loss = 0.08180624\n",
      "Iteration 86, loss = 0.08058975\n",
      "Iteration 40, loss = 0.08176754\n",
      "Iteration 87, loss = 0.08046963\n",
      "Iteration 41, loss = 0.08161458\n",
      "Iteration 88, loss = 0.08057666\n",
      "Iteration 42, loss = 0.08152893\n",
      "Iteration 89, loss = 0.08045698\n",
      "Iteration 43, loss = 0.08154894\n",
      "Iteration 90, loss = 0.08047263\n",
      "Iteration 44, loss = 0.08150464\n",
      "Iteration 91, loss = 0.08051526\n",
      "Iteration 45, loss = 0.08152323\n",
      "Iteration 92, loss = 0.08041944\n",
      "Iteration 46, loss = 0.08141922\n",
      "Iteration 93, loss = 0.08041395\n",
      "Iteration 47, loss = 0.08132589\n",
      "Iteration 94, loss = 0.08036000\n",
      "Iteration 48, loss = 0.08142374\n",
      "Iteration 95, loss = 0.08034604\n",
      "Iteration 49, loss = 0.08131580\n",
      "Iteration 96, loss = 0.08031600\n",
      "Iteration 50, loss = 0.08128928\n",
      "Iteration 97, loss = 0.08036027\n",
      "Iteration 51, loss = 0.08121735\n",
      "Iteration 98, loss = 0.08036960\n",
      "Iteration 52, loss = 0.08120562\n",
      "Iteration 99, loss = 0.08022221\n",
      "Iteration 53, loss = 0.08122816\n",
      "Iteration 100, loss = 0.08024060\n",
      "Iteration 54, loss = 0.08118985\n",
      "Iteration 101, loss = 0.08033560\n",
      "Iteration 55, loss = 0.08113636\n",
      "Iteration 102, loss = 0.08025525\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 0.08104338\n",
      "Iteration 57, loss = 0.08114624\n",
      "Iteration 58, loss = 0.08098501\n",
      "Iteration 59, loss = 0.08087805\n",
      "Iteration 60, loss = 0.08093600\n",
      "Iteration 61, loss = 0.08090047\n",
      "Iteration 62, loss = 0.08085395\n",
      "Iteration 63, loss = 0.08084214\n",
      "Iteration 64, loss = 0.08082123\n",
      "Iteration 65, loss = 0.08083556\n",
      "Iteration 66, loss = 0.08075113\n",
      "Iteration 67, loss = 0.08072154\n",
      "Iteration 68, loss = 0.08076945\n",
      "Iteration 69, loss = 0.08063253\n",
      "Iteration 70, loss = 0.08066213\n",
      "Iteration 71, loss = 0.08064707\n",
      "Iteration 72, loss = 0.08061540\n",
      "Iteration 73, loss = 0.08059890\n",
      "Iteration 74, loss = 0.08060004\n",
      "Iteration 75, loss = 0.08054095\n",
      "Iteration 76, loss = 0.08062421\n",
      "Iteration 77, loss = 0.08048390\n",
      "Iteration 78, loss = 0.08048940\n",
      "Iteration 79, loss = 0.08045106\n",
      "Iteration 80, loss = 0.08047482\n",
      "Iteration 81, loss = 0.08041390\n",
      "Iteration 82, loss = 0.08044946\n",
      "Iteration 83, loss = 0.08042431\n",
      "Iteration 84, loss = 0.08046251\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.11960283\n",
      "Iteration 1, loss = 0.19107919\n",
      "Iteration 1, loss = 0.11984623\n",
      "Iteration 1, loss = 0.11980321\n",
      "Iteration 2, loss = 0.09075178\n",
      "Iteration 2, loss = 0.09553466\n",
      "Iteration 2, loss = 0.09120576\n",
      "Iteration 2, loss = 0.09082693\n",
      "Iteration 3, loss = 0.08795454\n",
      "Iteration 3, loss = 0.08891545\n",
      "Iteration 3, loss = 0.09141725\n",
      "Iteration 3, loss = 0.08839654\n",
      "Iteration 4, loss = 0.08666877\n",
      "Iteration 4, loss = 0.08753887\n",
      "Iteration 4, loss = 0.08847214\n",
      "Iteration 4, loss = 0.08721360\n",
      "Iteration 5, loss = 0.08595508\n",
      "Iteration 5, loss = 0.08670806\n",
      "Iteration 5, loss = 0.08672391\n",
      "Iteration 5, loss = 0.08648296\n",
      "Iteration 6, loss = 0.08528659\n",
      "Iteration 6, loss = 0.08595587\n",
      "Iteration 6, loss = 0.08591558\n",
      "Iteration 6, loss = 0.08603942\n",
      "Iteration 7, loss = 0.08485532\n",
      "Iteration 7, loss = 0.08550777\n",
      "Iteration 7, loss = 0.08535920\n",
      "Iteration 7, loss = 0.08538033\n",
      "Iteration 8, loss = 0.08461111\n",
      "Iteration 8, loss = 0.08525769\n",
      "Iteration 8, loss = 0.08494448\n",
      "Iteration 8, loss = 0.08499515\n",
      "Iteration 9, loss = 0.08441187\n",
      "Iteration 9, loss = 0.08487064\n",
      "Iteration 9, loss = 0.08475436\n",
      "Iteration 9, loss = 0.08481052\n",
      "Iteration 10, loss = 0.08417380\n",
      "Iteration 10, loss = 0.08468438\n",
      "Iteration 10, loss = 0.08445623\n",
      "Iteration 10, loss = 0.08454528\n",
      "Iteration 11, loss = 0.08420301\n",
      "Iteration 11, loss = 0.08452062\n",
      "Iteration 11, loss = 0.08419306\n",
      "Iteration 11, loss = 0.08432131\n",
      "Iteration 12, loss = 0.08394159\n",
      "Iteration 12, loss = 0.08424370\n",
      "Iteration 12, loss = 0.08399909\n",
      "Iteration 12, loss = 0.08425679\n",
      "Iteration 13, loss = 0.08384645\n",
      "Iteration 13, loss = 0.08409633\n",
      "Iteration 13, loss = 0.08381916\n",
      "Iteration 13, loss = 0.08407872\n",
      "Iteration 14, loss = 0.08385457\n",
      "Iteration 14, loss = 0.08398714\n",
      "Iteration 14, loss = 0.08366420\n",
      "Iteration 14, loss = 0.08404347\n",
      "Iteration 15, loss = 0.08362992\n",
      "Iteration 15, loss = 0.08393831\n",
      "Iteration 15, loss = 0.08356784\n",
      "Iteration 15, loss = 0.08379846\n",
      "Iteration 16, loss = 0.08356241\n",
      "Iteration 16, loss = 0.08379561\n",
      "Iteration 16, loss = 0.08339618\n",
      "Iteration 16, loss = 0.08365539\n",
      "Iteration 17, loss = 0.08339950\n",
      "Iteration 17, loss = 0.08364661\n",
      "Iteration 17, loss = 0.08339929\n",
      "Iteration 17, loss = 0.08373307\n",
      "Iteration 18, loss = 0.08343055\n",
      "Iteration 18, loss = 0.08358650\n",
      "Iteration 18, loss = 0.08323746\n",
      "Iteration 19, loss = 0.08335747\n",
      "Iteration 18, loss = 0.08359295\n",
      "Iteration 19, loss = 0.08361629\n",
      "Iteration 19, loss = 0.08317090\n",
      "Iteration 20, loss = 0.08328423\n",
      "Iteration 19, loss = 0.08345717\n",
      "Iteration 20, loss = 0.08350074\n",
      "Iteration 20, loss = 0.08317562\n",
      "Iteration 21, loss = 0.08328485\n",
      "Iteration 20, loss = 0.08342020\n",
      "Iteration 21, loss = 0.08339955\n",
      "Iteration 21, loss = 0.08305002\n",
      "Iteration 22, loss = 0.08321691\n",
      "Iteration 21, loss = 0.08325237\n",
      "Iteration 22, loss = 0.08342417\n",
      "Iteration 23, loss = 0.08310225\n",
      "Iteration 22, loss = 0.08323637\n",
      "Iteration 22, loss = 0.08300581\n",
      "Iteration 23, loss = 0.08313339\n",
      "Iteration 23, loss = 0.08316599\n",
      "Iteration 24, loss = 0.08296117\n",
      "Iteration 23, loss = 0.08295548\n",
      "Iteration 24, loss = 0.08313373\n",
      "Iteration 24, loss = 0.08310094\n",
      "Iteration 25, loss = 0.08289436\n",
      "Iteration 24, loss = 0.08274173\n",
      "Iteration 25, loss = 0.08300838\n",
      "Iteration 25, loss = 0.08302898\n",
      "Iteration 26, loss = 0.08287530\n",
      "Iteration 25, loss = 0.08270027\n",
      "Iteration 26, loss = 0.08297977\n",
      "Iteration 26, loss = 0.08283801\n",
      "Iteration 26, loss = 0.08258484\n",
      "Iteration 27, loss = 0.08292128\n",
      "Iteration 27, loss = 0.08298847\n",
      "Iteration 27, loss = 0.08288803\n",
      "Iteration 28, loss = 0.08272217\n",
      "Iteration 27, loss = 0.08265192\n",
      "Iteration 28, loss = 0.08294658\n",
      "Iteration 28, loss = 0.08291193\n",
      "Iteration 28, loss = 0.08256715\n",
      "Iteration 29, loss = 0.08273031\n",
      "Iteration 29, loss = 0.08288993\n",
      "Iteration 29, loss = 0.08285960\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.08269770\n",
      "Iteration 29, loss = 0.08245416\n",
      "Iteration 30, loss = 0.08283999\n",
      "Iteration 1, loss = 0.11483651\n",
      "Iteration 31, loss = 0.08257668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 0.08244706\n",
      "Iteration 31, loss = 0.08273999\n",
      "Iteration 2, loss = 0.09096395\n",
      "Iteration 32, loss = 0.08257140\n",
      "Iteration 31, loss = 0.08242389\n",
      "Iteration 3, loss = 0.08751248\n",
      "Iteration 32, loss = 0.08269046\n",
      "Iteration 33, loss = 0.08245077\n",
      "Iteration 32, loss = 0.08246621\n",
      "Iteration 34, loss = 0.08238261\n",
      "Iteration 4, loss = 0.08606131\n",
      "Iteration 33, loss = 0.08261913\n",
      "Iteration 33, loss = 0.08237723\n",
      "Iteration 35, loss = 0.08236211\n",
      "Iteration 5, loss = 0.08528634\n",
      "Iteration 34, loss = 0.08257914\n",
      "Iteration 34, loss = 0.08229706\n",
      "Iteration 36, loss = 0.08238877\n",
      "Iteration 6, loss = 0.08490473\n",
      "Iteration 35, loss = 0.08251264\n",
      "Iteration 35, loss = 0.08209608\n",
      "Iteration 7, loss = 0.08453131\n",
      "Iteration 37, loss = 0.08230492\n",
      "Iteration 36, loss = 0.08256532\n",
      "Iteration 36, loss = 0.08205230\n",
      "Iteration 8, loss = 0.08425544\n",
      "Iteration 38, loss = 0.08230536\n",
      "Iteration 37, loss = 0.08237869\n",
      "Iteration 37, loss = 0.08203121\n",
      "Iteration 9, loss = 0.08410426\n",
      "Iteration 39, loss = 0.08215402\n",
      "Iteration 38, loss = 0.08236733\n",
      "Iteration 38, loss = 0.08203169\n",
      "Iteration 10, loss = 0.08396580\n",
      "Iteration 39, loss = 0.08218630\n",
      "Iteration 40, loss = 0.08212461\n",
      "Iteration 39, loss = 0.08198611\n",
      "Iteration 11, loss = 0.08375482\n",
      "Iteration 40, loss = 0.08211811\n",
      "Iteration 41, loss = 0.08204280\n",
      "Iteration 40, loss = 0.08187340\n",
      "Iteration 12, loss = 0.08369602\n",
      "Iteration 41, loss = 0.08199543\n",
      "Iteration 42, loss = 0.08200614\n",
      "Iteration 41, loss = 0.08180557\n",
      "Iteration 13, loss = 0.08336211\n",
      "Iteration 42, loss = 0.08202555\n",
      "Iteration 43, loss = 0.08202141\n",
      "Iteration 42, loss = 0.08179288\n",
      "Iteration 14, loss = 0.08339250\n",
      "Iteration 43, loss = 0.08198445\n",
      "Iteration 44, loss = 0.08194377\n",
      "Iteration 43, loss = 0.08171230\n",
      "Iteration 15, loss = 0.08328664\n",
      "Iteration 44, loss = 0.08197500\n",
      "Iteration 45, loss = 0.08190456\n",
      "Iteration 44, loss = 0.08163907\n",
      "Iteration 16, loss = 0.08324271\n",
      "Iteration 45, loss = 0.08193602\n",
      "Iteration 46, loss = 0.08180493\n",
      "Iteration 45, loss = 0.08163132\n",
      "Iteration 17, loss = 0.08307412\n",
      "Iteration 46, loss = 0.08190733\n",
      "Iteration 47, loss = 0.08177492\n",
      "Iteration 46, loss = 0.08151352\n",
      "Iteration 47, loss = 0.08185215\n",
      "Iteration 18, loss = 0.08315274\n",
      "Iteration 48, loss = 0.08171850\n",
      "Iteration 47, loss = 0.08159808\n",
      "Iteration 48, loss = 0.08177364\n",
      "Iteration 19, loss = 0.08298231\n",
      "Iteration 49, loss = 0.08163286\n",
      "Iteration 49, loss = 0.08184813\n",
      "Iteration 48, loss = 0.08153832\n",
      "Iteration 20, loss = 0.08289175\n",
      "Iteration 50, loss = 0.08160035\n",
      "Iteration 50, loss = 0.08177023\n",
      "Iteration 49, loss = 0.08142540\n",
      "Iteration 21, loss = 0.08275643\n",
      "Iteration 51, loss = 0.08162126\n",
      "Iteration 51, loss = 0.08174453\n",
      "Iteration 50, loss = 0.08141957\n",
      "Iteration 22, loss = 0.08268257\n",
      "Iteration 52, loss = 0.08154428\n",
      "Iteration 52, loss = 0.08174370\n",
      "Iteration 23, loss = 0.08261529\n",
      "Iteration 51, loss = 0.08137766\n",
      "Iteration 53, loss = 0.08152801\n",
      "Iteration 53, loss = 0.08167483\n",
      "Iteration 24, loss = 0.08251977\n",
      "Iteration 52, loss = 0.08143653\n",
      "Iteration 54, loss = 0.08152816\n",
      "Iteration 54, loss = 0.08165287\n",
      "Iteration 25, loss = 0.08228394\n",
      "Iteration 53, loss = 0.08136068\n",
      "Iteration 55, loss = 0.08143596\n",
      "Iteration 26, loss = 0.08233991\n",
      "Iteration 55, loss = 0.08161079\n",
      "Iteration 54, loss = 0.08130371\n",
      "Iteration 27, loss = 0.08227215\n",
      "Iteration 56, loss = 0.08137223\n",
      "Iteration 56, loss = 0.08158391\n",
      "Iteration 55, loss = 0.08140484\n",
      "Iteration 28, loss = 0.08209584\n",
      "Iteration 29, loss = 0.08211227\n",
      "Iteration 57, loss = 0.08151185\n",
      "Iteration 57, loss = 0.08135287\n",
      "Iteration 56, loss = 0.08122404\n",
      "Iteration 30, loss = 0.08201083\n",
      "Iteration 58, loss = 0.08147479\n",
      "Iteration 31, loss = 0.08196471\n",
      "Iteration 58, loss = 0.08125394\n",
      "Iteration 57, loss = 0.08124923\n",
      "Iteration 32, loss = 0.08190924\n",
      "Iteration 59, loss = 0.08155725\n",
      "Iteration 59, loss = 0.08127574\n",
      "Iteration 33, loss = 0.08204742\n",
      "Iteration 58, loss = 0.08124377\n",
      "Iteration 34, loss = 0.08184420\n",
      "Iteration 60, loss = 0.08149830\n",
      "Iteration 60, loss = 0.08130142\n",
      "Iteration 35, loss = 0.08183332\n",
      "Iteration 59, loss = 0.08119249\n",
      "Iteration 36, loss = 0.08183615\n",
      "Iteration 61, loss = 0.08144216\n",
      "Iteration 61, loss = 0.08122051\n",
      "Iteration 37, loss = 0.08174871\n",
      "Iteration 60, loss = 0.08117845\n",
      "Iteration 38, loss = 0.08172885\n",
      "Iteration 62, loss = 0.08143804\n",
      "Iteration 62, loss = 0.08118713\n",
      "Iteration 39, loss = 0.08162906\n",
      "Iteration 61, loss = 0.08109801\n",
      "Iteration 40, loss = 0.08160209\n",
      "Iteration 63, loss = 0.08145456\n",
      "Iteration 41, loss = 0.08166575\n",
      "Iteration 63, loss = 0.08134840\n",
      "Iteration 62, loss = 0.08113600\n",
      "Iteration 42, loss = 0.08165579\n",
      "Iteration 64, loss = 0.08143530\n",
      "Iteration 43, loss = 0.08158559\n",
      "Iteration 64, loss = 0.08117954\n",
      "Iteration 63, loss = 0.08106005\n",
      "Iteration 44, loss = 0.08161127\n",
      "Iteration 65, loss = 0.08134347\n",
      "Iteration 45, loss = 0.08145145\n",
      "Iteration 65, loss = 0.08110123\n",
      "Iteration 64, loss = 0.08109523\n",
      "Iteration 46, loss = 0.08142804\n",
      "Iteration 47, loss = 0.08142275\n",
      "Iteration 66, loss = 0.08134878\n",
      "Iteration 66, loss = 0.08107900\n",
      "Iteration 48, loss = 0.08143750\n",
      "Iteration 65, loss = 0.08102189\n",
      "Iteration 49, loss = 0.08142562\n",
      "Iteration 67, loss = 0.08133282\n",
      "Iteration 67, loss = 0.08113857\n",
      "Iteration 50, loss = 0.08146015\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 66, loss = 0.08098137\n",
      "Iteration 68, loss = 0.08103435\n",
      "Iteration 68, loss = 0.08129588\n",
      "Iteration 67, loss = 0.08107725\n",
      "Iteration 69, loss = 0.08115203\n",
      "Iteration 69, loss = 0.08130634\n",
      "Iteration 68, loss = 0.08099211\n",
      "Iteration 70, loss = 0.08103875\n",
      "Iteration 70, loss = 0.08119025\n",
      "Iteration 69, loss = 0.08097595\n",
      "Iteration 71, loss = 0.08112257\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 71, loss = 0.08132299\n",
      "Iteration 70, loss = 0.08097813\n",
      "Iteration 71, loss = 0.08091995\n",
      "Iteration 72, loss = 0.08120653\n",
      "Iteration 72, loss = 0.08090590\n",
      "Iteration 73, loss = 0.08122537\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 73, loss = 0.08084390\n",
      "Iteration 74, loss = 0.08078545\n",
      "Iteration 75, loss = 0.08074126\n",
      "Iteration 76, loss = 0.08088520\n",
      "Iteration 77, loss = 0.08079159\n",
      "Iteration 78, loss = 0.08069797\n",
      "Iteration 79, loss = 0.08074707\n",
      "Iteration 80, loss = 0.08075726\n",
      "Iteration 81, loss = 0.08073011\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.11965383\n",
      "Iteration 1, loss = 0.12004854\n",
      "Iteration 1, loss = 0.15852129\n",
      "Iteration 1, loss = 0.11947508\n",
      "Iteration 2, loss = 0.09007362\n",
      "Iteration 2, loss = 0.09060190\n",
      "Iteration 2, loss = 0.09205848\n",
      "Iteration 2, loss = 0.09001424\n",
      "Iteration 3, loss = 0.08754991\n",
      "Iteration 3, loss = 0.08813974\n",
      "Iteration 3, loss = 0.08875504\n",
      "Iteration 3, loss = 0.08811231\n",
      "Iteration 4, loss = 0.08707519\n",
      "Iteration 4, loss = 0.08653376\n",
      "Iteration 4, loss = 0.08739167\n",
      "Iteration 4, loss = 0.08704536\n",
      "Iteration 5, loss = 0.08640772\n",
      "Iteration 5, loss = 0.08596822\n",
      "Iteration 5, loss = 0.08654212\n",
      "Iteration 5, loss = 0.08645001\n",
      "Iteration 6, loss = 0.08581693\n",
      "Iteration 6, loss = 0.08543345\n",
      "Iteration 6, loss = 0.08607872\n",
      "Iteration 6, loss = 0.08607819\n",
      "Iteration 7, loss = 0.08544711\n",
      "Iteration 7, loss = 0.08508887\n",
      "Iteration 7, loss = 0.08576198\n",
      "Iteration 7, loss = 0.08563859\n",
      "Iteration 8, loss = 0.08527408\n",
      "Iteration 8, loss = 0.08489076\n",
      "Iteration 8, loss = 0.08551596\n",
      "Iteration 8, loss = 0.08523357\n",
      "Iteration 9, loss = 0.08471246\n",
      "Iteration 9, loss = 0.08495364\n",
      "Iteration 9, loss = 0.08545388\n",
      "Iteration 9, loss = 0.08508011\n",
      "Iteration 10, loss = 0.08449180\n",
      "Iteration 10, loss = 0.08489682\n",
      "Iteration 10, loss = 0.08488843\n",
      "Iteration 10, loss = 0.08532214\n",
      "Iteration 11, loss = 0.08454420\n",
      "Iteration 11, loss = 0.08478080\n",
      "Iteration 11, loss = 0.08517061\n",
      "Iteration 11, loss = 0.08459302\n",
      "Iteration 12, loss = 0.08429557\n",
      "Iteration 12, loss = 0.08458781\n",
      "Iteration 12, loss = 0.08498668\n",
      "Iteration 12, loss = 0.08452273\n",
      "Iteration 13, loss = 0.08420612\n",
      "Iteration 13, loss = 0.08490102\n",
      "Iteration 13, loss = 0.08443115\n",
      "Iteration 14, loss = 0.08482988\n",
      "Iteration 13, loss = 0.08436215\n",
      "Iteration 14, loss = 0.08422073\n",
      "Iteration 14, loss = 0.08438341\n",
      "Iteration 15, loss = 0.08476137\n",
      "Iteration 15, loss = 0.08400878\n",
      "Iteration 14, loss = 0.08427146\n",
      "Iteration 15, loss = 0.08437836\n",
      "Iteration 16, loss = 0.08466222\n",
      "Iteration 16, loss = 0.08390438\n",
      "Iteration 16, loss = 0.08425277\n",
      "Iteration 15, loss = 0.08404951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.08464628\n",
      "Iteration 17, loss = 0.08379204\n",
      "Iteration 18, loss = 0.08454217\n",
      "Iteration 17, loss = 0.08410229\n",
      "Iteration 16, loss = 0.08389398\n",
      "Iteration 18, loss = 0.08381779\n",
      "Iteration 19, loss = 0.08445841\n",
      "Iteration 18, loss = 0.08398289\n",
      "Iteration 17, loss = 0.08397816\n",
      "Iteration 19, loss = 0.08374549\n",
      "Iteration 20, loss = 0.08445574\n",
      "Iteration 19, loss = 0.08405497\n",
      "Iteration 20, loss = 0.08366856\n",
      "Iteration 21, loss = 0.08432713\n",
      "Iteration 18, loss = 0.08382361\n",
      "Iteration 20, loss = 0.08392150\n",
      "Iteration 21, loss = 0.08372048\n",
      "Iteration 22, loss = 0.08426623\n",
      "Iteration 19, loss = 0.08371210\n",
      "Iteration 22, loss = 0.08363326\n",
      "Iteration 21, loss = 0.08382737\n",
      "Iteration 23, loss = 0.08427146\n",
      "Iteration 20, loss = 0.08369807\n",
      "Iteration 23, loss = 0.08351181\n",
      "Iteration 22, loss = 0.08384371\n",
      "Iteration 24, loss = 0.08407840\n",
      "Iteration 21, loss = 0.08347867\n",
      "Iteration 24, loss = 0.08338951\n",
      "Iteration 25, loss = 0.08402120\n",
      "Iteration 23, loss = 0.08360173\n",
      "Iteration 26, loss = 0.08381383\n",
      "Iteration 25, loss = 0.08332687\n",
      "Iteration 22, loss = 0.08358124\n",
      "Iteration 24, loss = 0.08361309\n",
      "Iteration 27, loss = 0.08393295\n",
      "Iteration 26, loss = 0.08327024\n",
      "Iteration 23, loss = 0.08340241\n",
      "Iteration 25, loss = 0.08348749\n",
      "Iteration 28, loss = 0.08375398\n",
      "Iteration 27, loss = 0.08329318\n",
      "Iteration 24, loss = 0.08336877\n",
      "Iteration 26, loss = 0.08351163\n",
      "Iteration 29, loss = 0.08361080\n",
      "Iteration 28, loss = 0.08312863\n",
      "Iteration 27, loss = 0.08354426\n",
      "Iteration 30, loss = 0.08359910\n",
      "Iteration 25, loss = 0.08332822\n",
      "Iteration 29, loss = 0.08312890\n",
      "Iteration 31, loss = 0.08353705\n",
      "Iteration 28, loss = 0.08344726\n",
      "Iteration 26, loss = 0.08315101\n",
      "Iteration 30, loss = 0.08309034\n",
      "Iteration 32, loss = 0.08358190\n",
      "Iteration 29, loss = 0.08342973\n",
      "Iteration 27, loss = 0.08312753\n",
      "Iteration 31, loss = 0.08298045\n",
      "Iteration 33, loss = 0.08349219\n",
      "Iteration 30, loss = 0.08335770\n",
      "Iteration 32, loss = 0.08293026\n",
      "Iteration 34, loss = 0.08335459\n",
      "Iteration 28, loss = 0.08312060\n",
      "Iteration 31, loss = 0.08322953\n",
      "Iteration 35, loss = 0.08325976\n",
      "Iteration 33, loss = 0.08283169\n",
      "Iteration 29, loss = 0.08305687\n",
      "Iteration 36, loss = 0.08317524\n",
      "Iteration 32, loss = 0.08321814\n",
      "Iteration 34, loss = 0.08275796\n",
      "Iteration 30, loss = 0.08295045\n",
      "Iteration 37, loss = 0.08315636\n",
      "Iteration 35, loss = 0.08271479\n",
      "Iteration 33, loss = 0.08314630\n",
      "Iteration 38, loss = 0.08314578\n",
      "Iteration 31, loss = 0.08293514\n",
      "Iteration 36, loss = 0.08276972\n",
      "Iteration 34, loss = 0.08310208\n",
      "Iteration 39, loss = 0.08305951\n",
      "Iteration 37, loss = 0.08266612\n",
      "Iteration 32, loss = 0.08292930\n",
      "Iteration 35, loss = 0.08305343\n",
      "Iteration 40, loss = 0.08300030\n",
      "Iteration 38, loss = 0.08269239\n",
      "Iteration 33, loss = 0.08292940\n",
      "Iteration 36, loss = 0.08319357\n",
      "Iteration 41, loss = 0.08289803\n",
      "Iteration 39, loss = 0.08249885\n",
      "Iteration 42, loss = 0.08295805\n",
      "Iteration 34, loss = 0.08274446\n",
      "Iteration 37, loss = 0.08300960\n",
      "Iteration 40, loss = 0.08252299\n",
      "Iteration 43, loss = 0.08285811\n",
      "Iteration 38, loss = 0.08297576\n",
      "Iteration 35, loss = 0.08264790\n",
      "Iteration 41, loss = 0.08245063\n",
      "Iteration 44, loss = 0.08280837\n",
      "Iteration 39, loss = 0.08289675\n",
      "Iteration 42, loss = 0.08244135\n",
      "Iteration 36, loss = 0.08259884\n",
      "Iteration 45, loss = 0.08279824\n",
      "Iteration 43, loss = 0.08242852\n",
      "Iteration 40, loss = 0.08284859\n",
      "Iteration 46, loss = 0.08273741\n",
      "Iteration 37, loss = 0.08263099\n",
      "Iteration 44, loss = 0.08242822\n",
      "Iteration 47, loss = 0.08276946\n",
      "Iteration 41, loss = 0.08273870\n",
      "Iteration 38, loss = 0.08249770\n",
      "Iteration 45, loss = 0.08240407\n",
      "Iteration 48, loss = 0.08276621\n",
      "Iteration 42, loss = 0.08278357\n",
      "Iteration 39, loss = 0.08237166\n",
      "Iteration 49, loss = 0.08263397\n",
      "Iteration 46, loss = 0.08238620\n",
      "Iteration 43, loss = 0.08274692\n",
      "Iteration 50, loss = 0.08268104\n",
      "Iteration 47, loss = 0.08234316\n",
      "Iteration 40, loss = 0.08243339\n",
      "Iteration 44, loss = 0.08277654\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.08256955\n",
      "Iteration 48, loss = 0.08231144\n",
      "Iteration 1, loss = 0.20402369\n",
      "Iteration 41, loss = 0.08236606\n",
      "Iteration 2, loss = 0.10862765\n",
      "Iteration 52, loss = 0.08265897\n",
      "Iteration 3, loss = 0.09353894\n",
      "Iteration 49, loss = 0.08224510\n",
      "Iteration 4, loss = 0.08919541\n",
      "Iteration 53, loss = 0.08260740\n",
      "Iteration 42, loss = 0.08230919\n",
      "Iteration 5, loss = 0.08726908\n",
      "Iteration 50, loss = 0.08221508\n",
      "Iteration 6, loss = 0.08631966\n",
      "Iteration 54, loss = 0.08258981\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.08235316\n",
      "Iteration 51, loss = 0.08232851\n",
      "Iteration 7, loss = 0.08575782\n",
      "Iteration 44, loss = 0.08222351\n",
      "Iteration 8, loss = 0.08528846\n",
      "Iteration 52, loss = 0.08222467\n",
      "Iteration 45, loss = 0.08226254\n",
      "Iteration 9, loss = 0.08513706\n",
      "Iteration 53, loss = 0.08219626\n",
      "Iteration 10, loss = 0.08489145\n",
      "Iteration 46, loss = 0.08216241\n",
      "Iteration 54, loss = 0.08220897\n",
      "Iteration 11, loss = 0.08466619\n",
      "Iteration 55, loss = 0.08219133\n",
      "Iteration 47, loss = 0.08222662\n",
      "Iteration 12, loss = 0.08446324\n",
      "Iteration 56, loss = 0.08213208\n",
      "Iteration 13, loss = 0.08437697\n",
      "Iteration 48, loss = 0.08211736\n",
      "Iteration 14, loss = 0.08431023\n",
      "Iteration 57, loss = 0.08208967\n",
      "Iteration 49, loss = 0.08212375\n",
      "Iteration 15, loss = 0.08403621\n",
      "Iteration 58, loss = 0.08202890\n",
      "Iteration 50, loss = 0.08205212\n",
      "Iteration 16, loss = 0.08395751\n",
      "Iteration 59, loss = 0.08205680\n",
      "Iteration 51, loss = 0.08209123\n",
      "Iteration 17, loss = 0.08382665\n",
      "Iteration 60, loss = 0.08205599\n",
      "Iteration 52, loss = 0.08209917\n",
      "Iteration 18, loss = 0.08370373\n",
      "Iteration 61, loss = 0.08204041\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.08201151\n",
      "Iteration 19, loss = 0.08372774\n",
      "Iteration 54, loss = 0.08205791\n",
      "Iteration 20, loss = 0.08362826\n",
      "Iteration 21, loss = 0.08344502\n",
      "Iteration 55, loss = 0.08204392\n",
      "Iteration 22, loss = 0.08334016\n",
      "Iteration 56, loss = 0.08192857\n",
      "Iteration 23, loss = 0.08331250\n",
      "Iteration 57, loss = 0.08202435\n",
      "Iteration 24, loss = 0.08317737\n",
      "Iteration 58, loss = 0.08200860\n",
      "Iteration 25, loss = 0.08314228\n",
      "Iteration 59, loss = 0.08186991\n",
      "Iteration 26, loss = 0.08303392\n",
      "Iteration 27, loss = 0.08298844\n",
      "Iteration 60, loss = 0.08193822\n",
      "Iteration 28, loss = 0.08284498\n",
      "Iteration 61, loss = 0.08187484\n",
      "Iteration 29, loss = 0.08278426\n",
      "Iteration 62, loss = 0.08185128\n",
      "Iteration 30, loss = 0.08272989\n",
      "Iteration 63, loss = 0.08182873\n",
      "Iteration 31, loss = 0.08258285\n",
      "Iteration 64, loss = 0.08175470\n",
      "Iteration 32, loss = 0.08268857\n",
      "Iteration 65, loss = 0.08178212\n",
      "Iteration 33, loss = 0.08254082\n",
      "Iteration 34, loss = 0.08250573\n",
      "Iteration 66, loss = 0.08182930\n",
      "Iteration 35, loss = 0.08248706\n",
      "Iteration 67, loss = 0.08170600\n",
      "Iteration 36, loss = 0.08242045\n",
      "Iteration 68, loss = 0.08171427\n",
      "Iteration 37, loss = 0.08231810\n",
      "Iteration 69, loss = 0.08169273\n",
      "Iteration 38, loss = 0.08226173\n",
      "Iteration 70, loss = 0.08172410\n",
      "Iteration 39, loss = 0.08228303\n",
      "Iteration 71, loss = 0.08171219\n",
      "Iteration 40, loss = 0.08213946\n",
      "Iteration 41, loss = 0.08215379\n",
      "Iteration 72, loss = 0.08179978\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.08212554\n",
      "Iteration 43, loss = 0.08222941\n",
      "Iteration 44, loss = 0.08208257\n",
      "Iteration 45, loss = 0.08202967\n",
      "Iteration 46, loss = 0.08197951\n",
      "Iteration 47, loss = 0.08204028\n",
      "Iteration 48, loss = 0.08208408\n",
      "Iteration 49, loss = 0.08203762\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12069640\n",
      "Iteration 1, loss = 0.11989152\n",
      "Iteration 1, loss = 0.15322202\n",
      "Iteration 1, loss = 0.12001144\n",
      "Iteration 2, loss = 0.09099836\n",
      "Iteration 2, loss = 0.09117972\n",
      "Iteration 2, loss = 0.09472389\n",
      "Iteration 2, loss = 0.09048139\n",
      "Iteration 3, loss = 0.08835304\n",
      "Iteration 3, loss = 0.08852524\n",
      "Iteration 3, loss = 0.08812583\n",
      "Iteration 3, loss = 0.09110940\n",
      "Iteration 4, loss = 0.08739564\n",
      "Iteration 4, loss = 0.08718776\n",
      "Iteration 4, loss = 0.08717008\n",
      "Iteration 4, loss = 0.08889742\n",
      "Iteration 5, loss = 0.08689021\n",
      "Iteration 5, loss = 0.08642101\n",
      "Iteration 5, loss = 0.08669924\n",
      "Iteration 5, loss = 0.08764026\n",
      "Iteration 6, loss = 0.08636841\n",
      "Iteration 6, loss = 0.08587765\n",
      "Iteration 6, loss = 0.08704742\n",
      "Iteration 6, loss = 0.08648828\n",
      "Iteration 7, loss = 0.08614996\n",
      "Iteration 7, loss = 0.08561005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.08617057\n",
      "Iteration 7, loss = 0.08659058\n",
      "Iteration 8, loss = 0.08601563\n",
      "Iteration 8, loss = 0.08543453\n",
      "Iteration 8, loss = 0.08592181\n",
      "Iteration 8, loss = 0.08624145\n",
      "Iteration 9, loss = 0.08575253\n",
      "Iteration 9, loss = 0.08525267\n",
      "Iteration 9, loss = 0.08589413\n",
      "Iteration 9, loss = 0.08611448\n",
      "Iteration 10, loss = 0.08569412\n",
      "Iteration 10, loss = 0.08507635\n",
      "Iteration 10, loss = 0.08591687\n",
      "Iteration 10, loss = 0.08583622\n",
      "Iteration 11, loss = 0.08514724\n",
      "Iteration 11, loss = 0.08561213\n",
      "Iteration 11, loss = 0.08562862\n",
      "Iteration 11, loss = 0.08569100\n",
      "Iteration 12, loss = 0.08486625\n",
      "Iteration 12, loss = 0.08538338\n",
      "Iteration 12, loss = 0.08563786\n",
      "Iteration 12, loss = 0.08555515\n",
      "Iteration 13, loss = 0.08479662\n",
      "Iteration 13, loss = 0.08525070\n",
      "Iteration 13, loss = 0.08561245\n",
      "Iteration 13, loss = 0.08546124\n",
      "Iteration 14, loss = 0.08487016\n",
      "Iteration 14, loss = 0.08519434\n",
      "Iteration 15, loss = 0.08465031\n",
      "Iteration 14, loss = 0.08552689\n",
      "Iteration 14, loss = 0.08525119\n",
      "Iteration 15, loss = 0.08510615\n",
      "Iteration 16, loss = 0.08456847\n",
      "Iteration 15, loss = 0.08539982\n",
      "Iteration 15, loss = 0.08525750\n",
      "Iteration 16, loss = 0.08501705\n",
      "Iteration 17, loss = 0.08447679\n",
      "Iteration 16, loss = 0.08509141\n",
      "Iteration 16, loss = 0.08521233\n",
      "Iteration 17, loss = 0.08487443\n",
      "Iteration 18, loss = 0.08439645\n",
      "Iteration 17, loss = 0.08509299\n",
      "Iteration 17, loss = 0.08534118\n",
      "Iteration 19, loss = 0.08442635\n",
      "Iteration 18, loss = 0.08477116\n",
      "Iteration 18, loss = 0.08493816\n",
      "Iteration 20, loss = 0.08433613\n",
      "Iteration 18, loss = 0.08523413\n",
      "Iteration 19, loss = 0.08482098\n",
      "Iteration 21, loss = 0.08439712\n",
      "Iteration 19, loss = 0.08491968\n",
      "Iteration 19, loss = 0.08504765\n",
      "Iteration 20, loss = 0.08472090\n",
      "Iteration 22, loss = 0.08428235\n",
      "Iteration 20, loss = 0.08494766\n",
      "Iteration 20, loss = 0.08507548\n",
      "Iteration 21, loss = 0.08467506\n",
      "Iteration 23, loss = 0.08411655\n",
      "Iteration 21, loss = 0.08483415\n",
      "Iteration 21, loss = 0.08493219\n",
      "Iteration 24, loss = 0.08398026\n",
      "Iteration 22, loss = 0.08459307\n",
      "Iteration 22, loss = 0.08477859\n",
      "Iteration 25, loss = 0.08404562\n",
      "Iteration 22, loss = 0.08500759\n",
      "Iteration 23, loss = 0.08438496\n",
      "Iteration 23, loss = 0.08474857\n",
      "Iteration 26, loss = 0.08399963\n",
      "Iteration 23, loss = 0.08484424\n",
      "Iteration 24, loss = 0.08443991\n",
      "Iteration 27, loss = 0.08407137\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.08454510\n",
      "Iteration 24, loss = 0.08483872\n",
      "Iteration 1, loss = 0.16072379\n",
      "Iteration 25, loss = 0.08431036\n",
      "Iteration 2, loss = 0.10431725\n",
      "Iteration 25, loss = 0.08453282\n",
      "Iteration 3, loss = 0.09725249\n",
      "Iteration 26, loss = 0.08427724\n",
      "Iteration 4, loss = 0.09409471\n",
      "Iteration 25, loss = 0.08478327\n",
      "Iteration 5, loss = 0.09106551\n",
      "Iteration 26, loss = 0.08442069\n",
      "Iteration 6, loss = 0.08911905\n",
      "Iteration 26, loss = 0.08459049\n",
      "Iteration 27, loss = 0.08438039\n",
      "Iteration 27, loss = 0.08447736\n",
      "Iteration 7, loss = 0.08792177\n",
      "Iteration 27, loss = 0.08460692\n",
      "Iteration 28, loss = 0.08427604\n",
      "Iteration 8, loss = 0.08736858\n",
      "Iteration 28, loss = 0.08448245\n",
      "Iteration 9, loss = 0.08697040\n",
      "Iteration 28, loss = 0.08455918\n",
      "Iteration 29, loss = 0.08421586\n",
      "Iteration 29, loss = 0.08440113\n",
      "Iteration 10, loss = 0.08660312\n",
      "Iteration 30, loss = 0.08420914\n",
      "Iteration 11, loss = 0.08632195\n",
      "Iteration 29, loss = 0.08456797\n",
      "Iteration 30, loss = 0.08439435\n",
      "Iteration 12, loss = 0.08605639\n",
      "Iteration 31, loss = 0.08412007\n",
      "Iteration 30, loss = 0.08441032\n",
      "Iteration 31, loss = 0.08442121\n",
      "Iteration 13, loss = 0.08591533\n",
      "Iteration 32, loss = 0.08403081\n",
      "Iteration 32, loss = 0.08450287\n",
      "Iteration 31, loss = 0.08443353\n",
      "Iteration 14, loss = 0.08564712\n",
      "Iteration 33, loss = 0.08404361\n",
      "Iteration 15, loss = 0.08545421\n",
      "Iteration 33, loss = 0.08438725\n",
      "Iteration 32, loss = 0.08440929\n",
      "Iteration 16, loss = 0.08541016\n",
      "Iteration 34, loss = 0.08405447\n",
      "Iteration 34, loss = 0.08436359\n",
      "Iteration 33, loss = 0.08436138\n",
      "Iteration 17, loss = 0.08537596\n",
      "Iteration 35, loss = 0.08398480\n",
      "Iteration 35, loss = 0.08419971\n",
      "Iteration 34, loss = 0.08425611\n",
      "Iteration 18, loss = 0.08517907\n",
      "Iteration 36, loss = 0.08404825\n",
      "Iteration 36, loss = 0.08418205\n",
      "Iteration 35, loss = 0.08414241\n",
      "Iteration 19, loss = 0.08507161\n",
      "Iteration 37, loss = 0.08390383\n",
      "Iteration 37, loss = 0.08417830\n",
      "Iteration 20, loss = 0.08490771\n",
      "Iteration 36, loss = 0.08412635\n",
      "Iteration 21, loss = 0.08491097\n",
      "Iteration 38, loss = 0.08390923\n",
      "Iteration 38, loss = 0.08419241\n",
      "Iteration 37, loss = 0.08417222\n",
      "Iteration 22, loss = 0.08477728\n",
      "Iteration 39, loss = 0.08417564\n",
      "Iteration 39, loss = 0.08382734\n",
      "Iteration 38, loss = 0.08409224\n",
      "Iteration 23, loss = 0.08473312\n",
      "Iteration 40, loss = 0.08408744\n",
      "Iteration 40, loss = 0.08378911\n",
      "Iteration 24, loss = 0.08473919\n",
      "Iteration 39, loss = 0.08394040\n",
      "Iteration 41, loss = 0.08410045\n",
      "Iteration 25, loss = 0.08463080\n",
      "Iteration 41, loss = 0.08380472\n",
      "Iteration 40, loss = 0.08394566\n",
      "Iteration 26, loss = 0.08460913\n",
      "Iteration 42, loss = 0.08410128\n",
      "Iteration 42, loss = 0.08376568\n",
      "Iteration 41, loss = 0.08390540\n",
      "Iteration 27, loss = 0.08457172\n",
      "Iteration 43, loss = 0.08408605\n",
      "Iteration 43, loss = 0.08377281\n",
      "Iteration 28, loss = 0.08457579\n",
      "Iteration 42, loss = 0.08379827\n",
      "Iteration 44, loss = 0.08399188\n",
      "Iteration 29, loss = 0.08444245\n",
      "Iteration 44, loss = 0.08378506\n",
      "Iteration 43, loss = 0.08393292\n",
      "Iteration 30, loss = 0.08453360\n",
      "Iteration 45, loss = 0.08397249\n",
      "Iteration 44, loss = 0.08377936\n",
      "Iteration 45, loss = 0.08380686\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.08438134\n",
      "Iteration 46, loss = 0.08392683\n",
      "Iteration 45, loss = 0.08385326\n",
      "Iteration 32, loss = 0.08437492\n",
      "Iteration 47, loss = 0.08400973\n",
      "Iteration 33, loss = 0.08434685\n",
      "Iteration 46, loss = 0.08368761\n",
      "Iteration 48, loss = 0.08395150\n",
      "Iteration 34, loss = 0.08437257\n",
      "Iteration 47, loss = 0.08376304\n",
      "Iteration 49, loss = 0.08390196\n",
      "Iteration 35, loss = 0.08423803\n",
      "Iteration 48, loss = 0.08366943\n",
      "Iteration 50, loss = 0.08389094\n",
      "Iteration 36, loss = 0.08437144\n",
      "Iteration 51, loss = 0.08384446\n",
      "Iteration 49, loss = 0.08375638\n",
      "Iteration 37, loss = 0.08423680\n",
      "Iteration 52, loss = 0.08393230\n",
      "Iteration 50, loss = 0.08370193\n",
      "Iteration 38, loss = 0.08418961\n",
      "Iteration 53, loss = 0.08383941\n",
      "Iteration 51, loss = 0.08362421\n",
      "Iteration 39, loss = 0.08411659\n",
      "Iteration 54, loss = 0.08386863\n",
      "Iteration 40, loss = 0.08422718\n",
      "Iteration 52, loss = 0.08369759\n",
      "Iteration 55, loss = 0.08395713\n",
      "Iteration 41, loss = 0.08423793\n",
      "Iteration 53, loss = 0.08363228\n",
      "Iteration 42, loss = 0.08409741\n",
      "Iteration 56, loss = 0.08378697\n",
      "Iteration 54, loss = 0.08369175\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.08414812\n",
      "Iteration 57, loss = 0.08379806\n",
      "Iteration 44, loss = 0.08405684\n",
      "Iteration 58, loss = 0.08384454\n",
      "Iteration 45, loss = 0.08410377\n",
      "Iteration 59, loss = 0.08375045\n",
      "Iteration 46, loss = 0.08401273\n",
      "Iteration 60, loss = 0.08374844\n",
      "Iteration 47, loss = 0.08408206\n",
      "Iteration 48, loss = 0.08400158\n",
      "Iteration 61, loss = 0.08368400\n",
      "Iteration 49, loss = 0.08410182\n",
      "Iteration 62, loss = 0.08369097\n",
      "Iteration 50, loss = 0.08397742\n",
      "Iteration 63, loss = 0.08369856\n",
      "Iteration 51, loss = 0.08399130\n",
      "Iteration 64, loss = 0.08370907\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.08391166\n",
      "Iteration 53, loss = 0.08397787\n",
      "Iteration 54, loss = 0.08392786\n",
      "Iteration 55, loss = 0.08404602\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12172990\n",
      "Iteration 1, loss = 0.12241447\n",
      "Iteration 1, loss = 0.15578724\n",
      "Iteration 1, loss = 0.12190723\n",
      "Iteration 2, loss = 0.09321796\n",
      "Iteration 2, loss = 0.09373576\n",
      "Iteration 2, loss = 0.09323963\n",
      "Iteration 2, loss = 0.09760375\n",
      "Iteration 3, loss = 0.09087297\n",
      "Iteration 3, loss = 0.09050209\n",
      "Iteration 3, loss = 0.09409511\n",
      "Iteration 3, loss = 0.09067813\n",
      "Iteration 4, loss = 0.08947772\n",
      "Iteration 4, loss = 0.08978796\n",
      "Iteration 4, loss = 0.09218776\n",
      "Iteration 4, loss = 0.08959178\n",
      "Iteration 5, loss = 0.08930639\n",
      "Iteration 5, loss = 0.08898019\n",
      "Iteration 5, loss = 0.09111852\n",
      "Iteration 5, loss = 0.08916432\n",
      "Iteration 6, loss = 0.08888675\n",
      "Iteration 6, loss = 0.08864765\n",
      "Iteration 6, loss = 0.09058002\n",
      "Iteration 6, loss = 0.08895507\n",
      "Iteration 7, loss = 0.08872560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.08849145\n",
      "Iteration 7, loss = 0.09006217\n",
      "Iteration 7, loss = 0.08871996\n",
      "Iteration 8, loss = 0.08871721\n",
      "Iteration 8, loss = 0.08853266\n",
      "Iteration 8, loss = 0.08838149\n",
      "Iteration 8, loss = 0.08964751\n",
      "Iteration 9, loss = 0.08857337\n",
      "Iteration 9, loss = 0.08854386\n",
      "Iteration 9, loss = 0.08946116\n",
      "Iteration 9, loss = 0.08832843\n",
      "Iteration 10, loss = 0.08845739\n",
      "Iteration 10, loss = 0.08855660\n",
      "Iteration 10, loss = 0.08913139\n",
      "Iteration 10, loss = 0.08818371\n",
      "Iteration 11, loss = 0.08828160\n",
      "Iteration 11, loss = 0.08849614\n",
      "Iteration 11, loss = 0.08830580\n",
      "Iteration 11, loss = 0.08892726\n",
      "Iteration 12, loss = 0.08828978\n",
      "Iteration 12, loss = 0.08827552\n",
      "Iteration 12, loss = 0.08803205\n",
      "Iteration 13, loss = 0.08822315\n",
      "Iteration 12, loss = 0.08886428\n",
      "Iteration 13, loss = 0.08818682\n",
      "Iteration 14, loss = 0.08809873\n",
      "Iteration 13, loss = 0.08801781\n",
      "Iteration 13, loss = 0.08877937\n",
      "Iteration 14, loss = 0.08821437\n",
      "Iteration 15, loss = 0.08794315\n",
      "Iteration 14, loss = 0.08806121\n",
      "Iteration 15, loss = 0.08813400\n",
      "Iteration 14, loss = 0.08847336\n",
      "Iteration 16, loss = 0.08780793\n",
      "Iteration 16, loss = 0.08803476\n",
      "Iteration 15, loss = 0.08782925\n",
      "Iteration 15, loss = 0.08854441\n",
      "Iteration 17, loss = 0.08794287\n",
      "Iteration 17, loss = 0.08789651\n",
      "Iteration 16, loss = 0.08782513\n",
      "Iteration 16, loss = 0.08840750\n",
      "Iteration 18, loss = 0.08783951\n",
      "Iteration 18, loss = 0.08773397\n",
      "Iteration 17, loss = 0.08773041\n",
      "Iteration 17, loss = 0.08841744\n",
      "Iteration 19, loss = 0.08767980\n",
      "Iteration 19, loss = 0.08785596\n",
      "Iteration 18, loss = 0.08760227\n",
      "Iteration 20, loss = 0.08778092\n",
      "Iteration 18, loss = 0.08822098\n",
      "Iteration 20, loss = 0.08776234\n",
      "Iteration 21, loss = 0.08759661\n",
      "Iteration 19, loss = 0.08768671\n",
      "Iteration 19, loss = 0.08823262\n",
      "Iteration 21, loss = 0.08766284\n",
      "Iteration 22, loss = 0.08767891\n",
      "Iteration 20, loss = 0.08761317\n",
      "Iteration 20, loss = 0.08831234\n",
      "Iteration 22, loss = 0.08768888\n",
      "Iteration 23, loss = 0.08752011\n",
      "Iteration 21, loss = 0.08764367\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.08817719\n",
      "Iteration 24, loss = 0.08745786\n",
      "Iteration 1, loss = 0.13648175\n",
      "Iteration 23, loss = 0.08744063\n",
      "Iteration 2, loss = 0.09580173\n",
      "Iteration 22, loss = 0.08815495\n",
      "Iteration 3, loss = 0.09202741\n",
      "Iteration 25, loss = 0.08745061\n",
      "Iteration 4, loss = 0.09019343\n",
      "Iteration 24, loss = 0.08756666\n",
      "Iteration 5, loss = 0.08965742\n",
      "Iteration 26, loss = 0.08728765\n",
      "Iteration 23, loss = 0.08808502\n",
      "Iteration 6, loss = 0.08930162\n",
      "Iteration 25, loss = 0.08733123\n",
      "Iteration 27, loss = 0.08734579\n",
      "Iteration 24, loss = 0.08785884\n",
      "Iteration 7, loss = 0.08905672\n",
      "Iteration 28, loss = 0.08731538\n",
      "Iteration 26, loss = 0.08738457\n",
      "Iteration 25, loss = 0.08787781\n",
      "Iteration 8, loss = 0.08880750\n",
      "Iteration 29, loss = 0.08737427\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.08741627\n",
      "Iteration 26, loss = 0.08774692\n",
      "Iteration 9, loss = 0.08861309\n",
      "Iteration 27, loss = 0.08780081\n",
      "Iteration 10, loss = 0.08845509\n",
      "Iteration 28, loss = 0.08728047\n",
      "Iteration 11, loss = 0.08833989\n",
      "Iteration 28, loss = 0.08774803\n",
      "Iteration 29, loss = 0.08735112\n",
      "Iteration 12, loss = 0.08803937\n",
      "Iteration 29, loss = 0.08766188\n",
      "Iteration 30, loss = 0.08721336\n",
      "Iteration 13, loss = 0.08804321\n",
      "Iteration 30, loss = 0.08768851\n",
      "Iteration 31, loss = 0.08709518\n",
      "Iteration 14, loss = 0.08795587\n",
      "Iteration 31, loss = 0.08774597\n",
      "Iteration 32, loss = 0.08711588\n",
      "Iteration 15, loss = 0.08776718\n",
      "Iteration 32, loss = 0.08778194\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.08702710\n",
      "Iteration 16, loss = 0.08779469\n",
      "Iteration 34, loss = 0.08699646\n",
      "Iteration 17, loss = 0.08751462\n",
      "Iteration 35, loss = 0.08694821\n",
      "Iteration 18, loss = 0.08742706\n",
      "Iteration 36, loss = 0.08704372\n",
      "Iteration 19, loss = 0.08729119\n",
      "Iteration 37, loss = 0.08688814\n",
      "Iteration 20, loss = 0.08736374\n",
      "Iteration 38, loss = 0.08693768\n",
      "Iteration 21, loss = 0.08729940\n",
      "Iteration 39, loss = 0.08683708\n",
      "Iteration 22, loss = 0.08716275\n",
      "Iteration 40, loss = 0.08681559\n",
      "Iteration 23, loss = 0.08716106\n",
      "Iteration 41, loss = 0.08678104\n",
      "Iteration 24, loss = 0.08708240\n",
      "Iteration 42, loss = 0.08678953\n",
      "Iteration 25, loss = 0.08718731\n",
      "Iteration 43, loss = 0.08679321\n",
      "Iteration 26, loss = 0.08693688\n",
      "Iteration 44, loss = 0.08685002\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.08698985\n",
      "Iteration 28, loss = 0.08693986\n",
      "Iteration 29, loss = 0.08690486\n",
      "Iteration 30, loss = 0.08691549\n",
      "Iteration 31, loss = 0.08694504\n",
      "Iteration 32, loss = 0.08688748\n",
      "Iteration 33, loss = 0.08674331\n",
      "Iteration 34, loss = 0.08684139\n",
      "Iteration 35, loss = 0.08685015\n",
      "Iteration 36, loss = 0.08675645\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.13344560\n",
      "Iteration 1, loss = 0.13436872\n",
      "Iteration 1, loss = 0.16471112\n",
      "Iteration 1, loss = 0.13307214\n",
      "Iteration 2, loss = 0.10670203\n",
      "Iteration 2, loss = 0.10642180\n",
      "Iteration 2, loss = 0.10636196\n",
      "Iteration 2, loss = 0.10558774\n",
      "Iteration 3, loss = 0.10516347\n",
      "Iteration 3, loss = 0.10497617\n",
      "Iteration 3, loss = 0.10494789\n",
      "Iteration 3, loss = 0.10360700\n",
      "Iteration 4, loss = 0.10418622\n",
      "Iteration 4, loss = 0.10443844\n",
      "Iteration 4, loss = 0.10420793\n",
      "Iteration 4, loss = 0.10185114\n",
      "Iteration 5, loss = 0.10374098\n",
      "Iteration 5, loss = 0.10399086\n",
      "Iteration 5, loss = 0.10368161\n",
      "Iteration 5, loss = 0.10069804\n",
      "Iteration 6, loss = 0.10332751\n",
      "Iteration 6, loss = 0.10357005\n",
      "Iteration 6, loss = 0.10339286\n",
      "Iteration 6, loss = 0.10011674\n",
      "Iteration 7, loss = 0.10315204\n",
      "Iteration 7, loss = 0.10337412\n",
      "Iteration 7, loss = 0.10280769\n",
      "Iteration 7, loss = 0.09964952\n",
      "Iteration 8, loss = 0.10291864\n",
      "Iteration 8, loss = 0.10141907\n",
      "Iteration 8, loss = 0.10321232\n",
      "Iteration 8, loss = 0.09932276\n",
      "Iteration 9, loss = 0.10271324\n",
      "Iteration 9, loss = 0.10046136\n",
      "Iteration 9, loss = 0.10289521\n",
      "Iteration 9, loss = 0.09917881\n",
      "Iteration 10, loss = 0.09972253\n",
      "Iteration 10, loss = 0.10243831\n",
      "Iteration 10, loss = 0.09903423\n",
      "Iteration 10, loss = 0.10275593\n",
      "Iteration 11, loss = 0.09915842\n",
      "Iteration 11, loss = 0.10237261\n",
      "Iteration 11, loss = 0.09876573\n",
      "Iteration 11, loss = 0.10254459\n",
      "Iteration 12, loss = 0.09892909\n",
      "Iteration 12, loss = 0.10179285\n",
      "Iteration 12, loss = 0.09869990\n",
      "Iteration 12, loss = 0.10159284\n",
      "Iteration 13, loss = 0.09871433\n",
      "Iteration 13, loss = 0.10040836\n",
      "Iteration 13, loss = 0.09865847\n",
      "Iteration 13, loss = 0.10051043\n",
      "Iteration 14, loss = 0.09839720\n",
      "Iteration 14, loss = 0.09939553\n",
      "Iteration 14, loss = 0.09839955\n",
      "Iteration 14, loss = 0.09933234\n",
      "Iteration 15, loss = 0.09862543\n",
      "Iteration 15, loss = 0.09840079\n",
      "Iteration 15, loss = 0.09824993\n",
      "Iteration 15, loss = 0.09858144\n",
      "Iteration 16, loss = 0.09822501\n",
      "Iteration 16, loss = 0.09816869\n",
      "Iteration 16, loss = 0.09809767\n",
      "Iteration 16, loss = 0.09818330\n",
      "Iteration 17, loss = 0.09813672\n",
      "Iteration 17, loss = 0.09777083\n",
      "Iteration 17, loss = 0.09820089\n",
      "Iteration 18, loss = 0.09782620\n",
      "Iteration 18, loss = 0.09751364\n",
      "Iteration 17, loss = 0.09783883\n",
      "Iteration 19, loss = 0.09765349\n",
      "Iteration 18, loss = 0.09804912\n",
      "Iteration 18, loss = 0.09761817\n",
      "Iteration 19, loss = 0.09740193\n",
      "Iteration 20, loss = 0.09763588\n",
      "Iteration 19, loss = 0.09785405\n",
      "Iteration 19, loss = 0.09756925\n",
      "Iteration 20, loss = 0.09722719\n",
      "Iteration 21, loss = 0.09744593\n",
      "Iteration 20, loss = 0.09800677\n",
      "Iteration 20, loss = 0.09745788\n",
      "Iteration 21, loss = 0.09710115\n",
      "Iteration 22, loss = 0.09738926\n",
      "Iteration 21, loss = 0.09730966\n",
      "Iteration 21, loss = 0.09778869\n",
      "Iteration 22, loss = 0.09701317\n",
      "Iteration 22, loss = 0.09733157\n",
      "Iteration 23, loss = 0.09730255\n",
      "Iteration 22, loss = 0.09779994\n",
      "Iteration 23, loss = 0.09706052\n",
      "Iteration 23, loss = 0.09684739\n",
      "Iteration 24, loss = 0.09706700\n",
      "Iteration 23, loss = 0.09754062\n",
      "Iteration 24, loss = 0.09667432\n",
      "Iteration 24, loss = 0.09711709\n",
      "Iteration 25, loss = 0.09708595\n",
      "Iteration 24, loss = 0.09739531\n",
      "Iteration 25, loss = 0.09661469\n",
      "Iteration 26, loss = 0.09689121\n",
      "Iteration 25, loss = 0.09691268\n",
      "Iteration 25, loss = 0.09731987\n",
      "Iteration 26, loss = 0.09666960\n",
      "Iteration 27, loss = 0.09703952\n",
      "Iteration 26, loss = 0.09702628\n",
      "Iteration 26, loss = 0.09706665\n",
      "Iteration 27, loss = 0.09662074\n",
      "Iteration 27, loss = 0.09701230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.09709754\n",
      "Iteration 28, loss = 0.09695141\n",
      "Iteration 28, loss = 0.09652802\n",
      "Iteration 28, loss = 0.09692864\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.09705981\n",
      "Iteration 29, loss = 0.09687398\n",
      "Iteration 29, loss = 0.09652528\n",
      "Iteration 1, loss = 0.12206424\n",
      "Iteration 29, loss = 0.09711228\n",
      "Iteration 2, loss = 0.10602973\n",
      "Iteration 30, loss = 0.09691126\n",
      "Iteration 30, loss = 0.09652971\n",
      "Iteration 30, loss = 0.09693562\n",
      "Iteration 3, loss = 0.10431945\n",
      "Iteration 31, loss = 0.09649597\n",
      "Iteration 31, loss = 0.09700648\n",
      "Iteration 31, loss = 0.09701562\n",
      "Iteration 4, loss = 0.10302967\n",
      "Iteration 32, loss = 0.09645449\n",
      "Iteration 32, loss = 0.09696064\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.09691175\n",
      "Iteration 5, loss = 0.10166075\n",
      "Iteration 33, loss = 0.09641592\n",
      "Iteration 34, loss = 0.09636413\n",
      "Iteration 33, loss = 0.09687876\n",
      "Iteration 6, loss = 0.10084075\n",
      "Iteration 35, loss = 0.09632164\n",
      "Iteration 34, loss = 0.09687251\n",
      "Iteration 7, loss = 0.10017517\n",
      "Iteration 35, loss = 0.09671576\n",
      "Iteration 36, loss = 0.09641221\n",
      "Iteration 8, loss = 0.09975520\n",
      "Iteration 9, loss = 0.09946615\n",
      "Iteration 36, loss = 0.09671756\n",
      "Iteration 37, loss = 0.09633431\n",
      "Iteration 10, loss = 0.09915700\n",
      "Iteration 37, loss = 0.09668635\n",
      "Iteration 38, loss = 0.09638556\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.09882768\n",
      "Iteration 38, loss = 0.09661702\n",
      "Iteration 12, loss = 0.09865732\n",
      "Iteration 39, loss = 0.09650955\n",
      "Iteration 13, loss = 0.09831487\n",
      "Iteration 40, loss = 0.09651636\n",
      "Iteration 14, loss = 0.09815615\n",
      "Iteration 41, loss = 0.09648394\n",
      "Iteration 15, loss = 0.09804684\n",
      "Iteration 42, loss = 0.09636777\n",
      "Iteration 16, loss = 0.09772472\n",
      "Iteration 43, loss = 0.09645983\n",
      "Iteration 17, loss = 0.09760357\n",
      "Iteration 44, loss = 0.09633611\n",
      "Iteration 18, loss = 0.09752937\n",
      "Iteration 45, loss = 0.09642164\n",
      "Iteration 19, loss = 0.09728143\n",
      "Iteration 46, loss = 0.09624185\n",
      "Iteration 20, loss = 0.09707832\n",
      "Iteration 21, loss = 0.09703554\n",
      "Iteration 47, loss = 0.09633339\n",
      "Iteration 22, loss = 0.09692227\n",
      "Iteration 48, loss = 0.09628128\n",
      "Iteration 23, loss = 0.09688559\n",
      "Iteration 49, loss = 0.09633353\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.09670992\n",
      "Iteration 25, loss = 0.09671091\n",
      "Iteration 26, loss = 0.09668293\n",
      "Iteration 27, loss = 0.09663461\n",
      "Iteration 28, loss = 0.09643864\n",
      "Iteration 29, loss = 0.09643442\n",
      "Iteration 30, loss = 0.09648749\n",
      "Iteration 31, loss = 0.09638087\n",
      "Iteration 32, loss = 0.09636917\n",
      "Iteration 33, loss = 0.09644679\n",
      "Iteration 34, loss = 0.09636174\n",
      "Iteration 35, loss = 0.09627125\n",
      "Iteration 36, loss = 0.09633696\n",
      "Iteration 37, loss = 0.09629831\n",
      "Iteration 38, loss = 0.09637481\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.976129885446911,\n",
       " 0.9761330986247165,\n",
       " 0.9760375355007664,\n",
       " 0.976121909502643,\n",
       " 0.9760415020273905,\n",
       " 0.9759852669591975]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "f1_scores = []\n",
    "for alpha in alphas:\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(30, 15, 5), \n",
    "                        alpha=alpha, \n",
    "                        max_iter=500, \n",
    "                        tol=1e-7,\n",
    "                        verbose=True)\n",
    "    accuracy = cross_val_score(clf, \n",
    "                               X_train, \n",
    "                               y_train, \n",
    "                               cv=5, \n",
    "                               scoring='f1', \n",
    "                               n_jobs=-1).mean()\n",
    "    f1_scores.append(accuracy)\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f930d9c8470>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEACAYAAAByG0uxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f930e5c1588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.xscale('log')\n",
    "plt.plot(alphas, f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.11640786\n",
      "Iteration 2, loss = 0.09015974\n",
      "Iteration 3, loss = 0.08728568\n",
      "Iteration 4, loss = 0.08610947\n",
      "Iteration 5, loss = 0.08557656\n",
      "Iteration 6, loss = 0.08497867\n",
      "Iteration 7, loss = 0.08486810\n",
      "Iteration 8, loss = 0.08455729\n",
      "Iteration 9, loss = 0.08432937\n",
      "Iteration 10, loss = 0.08406475\n",
      "Iteration 11, loss = 0.08386939\n",
      "Iteration 12, loss = 0.08379434\n",
      "Iteration 13, loss = 0.08362961\n",
      "Iteration 14, loss = 0.08360408\n",
      "Iteration 15, loss = 0.08337712\n",
      "Iteration 16, loss = 0.08317159\n",
      "Iteration 17, loss = 0.08306836\n",
      "Iteration 18, loss = 0.08297905\n",
      "Iteration 19, loss = 0.08279687\n",
      "Iteration 20, loss = 0.08266009\n",
      "Iteration 21, loss = 0.08266700\n",
      "Iteration 22, loss = 0.08255230\n",
      "Iteration 23, loss = 0.08241608\n",
      "Iteration 24, loss = 0.08237659\n",
      "Iteration 25, loss = 0.08224684\n",
      "Iteration 26, loss = 0.08220304\n",
      "Iteration 27, loss = 0.08206906\n",
      "Iteration 28, loss = 0.08210548\n",
      "Iteration 29, loss = 0.08196649\n",
      "Iteration 30, loss = 0.08191240\n",
      "Iteration 31, loss = 0.08192621\n",
      "Iteration 32, loss = 0.08189306\n",
      "Iteration 33, loss = 0.08183363\n",
      "Iteration 34, loss = 0.08177467\n",
      "Iteration 35, loss = 0.08168294\n",
      "Iteration 36, loss = 0.08164594\n",
      "Iteration 37, loss = 0.08159800\n",
      "Iteration 38, loss = 0.08161797\n",
      "Iteration 39, loss = 0.08151510\n",
      "Iteration 40, loss = 0.08140865\n",
      "Iteration 41, loss = 0.08148825\n",
      "Iteration 42, loss = 0.08141377\n",
      "Iteration 43, loss = 0.08142531\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 15, 5), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=1e-07, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(30, 15, 5), \n",
    "                    alpha=1e-5, \n",
    "                    max_iter=500, \n",
    "                    tol=1e-7,\n",
    "                    verbose=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_test_pred, columns=['category'])\n",
    "df.index.name = 'id'\n",
    "df.to_csv('../result/mlp_new.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
