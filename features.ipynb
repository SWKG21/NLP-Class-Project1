{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title  \\\n",
       "0  1001  2000              compactification geometry and duality   \n",
       "1  1002  2000  domain walls and massive gauged supergravity p...   \n",
       "2  1003  2000     comment on metric fluctuations in brane worlds   \n",
       "3  1004  2000         moving mirrors and thermodynamic paradoxes   \n",
       "4  1005  2000  bundles of chiral blocks and boundary conditio...   \n",
       "\n",
       "                       authors            journal  \\\n",
       "0            Paul S. Aspinwall                NaN   \n",
       "1  M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "2     Y.S. Myung, Gungwon Kang                NaN   \n",
       "3               Adam D. Helfer          Phys.Rev.   \n",
       "4      J. Fuchs, C. Schweigert                NaN   \n",
       "\n",
       "                                            abstract  \n",
       "0  these are notes based on lectures given at tas...  \n",
       "1  we point out that massive gauged supergravity ...  \n",
       "2  recently ivanov and volovich hep-th 9912242 cl...  \n",
       "3  quantum fields responding to moving mirrors ha...  \n",
       "4  proceedings of lie iii clausthal july 1999 var...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_info = pd.read_csv('data/node_information.csv', header=None)\n",
    "node_info.columns = ['id', 'year', 'title', 'authors', 'journal', 'abstract']\n",
    "node_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>link</th>\n",
       "      <th>rno1</th>\n",
       "      <th>rno2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "      <td>16827</td>\n",
       "      <td>15446</td>\n",
       "      <td>0.064373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "      <td>21154</td>\n",
       "      <td>18059</td>\n",
       "      <td>0.021211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "      <td>13074</td>\n",
       "      <td>16171</td>\n",
       "      <td>0.017202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "      <td>27486</td>\n",
       "      <td>9702</td>\n",
       "      <td>0.012634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "      <td>19856</td>\n",
       "      <td>8212</td>\n",
       "      <td>0.059588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2  link   rno1   rno2       sim\n",
       "0  9510123  9502114     1  16827  15446  0.064373\n",
       "1  9707075  9604178     1  21154  18059  0.021211\n",
       "2  9312155  9506142     0  13074  16171  0.017202\n",
       "3  9911255   302165     0  27486   9702  0.012634\n",
       "4  9701033   209076     0  19856   8212  0.059588"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_treated.csv', index_col=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>rno1</th>\n",
       "      <th>rno2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9807076</td>\n",
       "      <td>9807139</td>\n",
       "      <td>23774</td>\n",
       "      <td>23835</td>\n",
       "      <td>0.071870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109162</td>\n",
       "      <td>1182</td>\n",
       "      <td>5227</td>\n",
       "      <td>172</td>\n",
       "      <td>0.163040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9702187</td>\n",
       "      <td>9510135</td>\n",
       "      <td>20185</td>\n",
       "      <td>16838</td>\n",
       "      <td>0.138004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111048</td>\n",
       "      <td>110115</td>\n",
       "      <td>5621</td>\n",
       "      <td>5397</td>\n",
       "      <td>0.101857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9910176</td>\n",
       "      <td>9410073</td>\n",
       "      <td>27159</td>\n",
       "      <td>14643</td>\n",
       "      <td>0.091231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2   rno1   rno2       sim\n",
       "0  9807076  9807139  23774  23835  0.071870\n",
       "1   109162     1182   5227    172  0.163040\n",
       "2  9702187  9510135  20185  16838  0.138004\n",
       "3   111048   110115   5621   5397  0.101857\n",
       "4  9910176  9410073  27159  14643  0.091231"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test_treated.csv', index_col=0)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## year difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>link</th>\n",
       "      <th>rno1</th>\n",
       "      <th>rno2</th>\n",
       "      <th>sim</th>\n",
       "      <th>year1</th>\n",
       "      <th>year2</th>\n",
       "      <th>year_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "      <td>16827</td>\n",
       "      <td>15446</td>\n",
       "      <td>0.064373</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "      <td>21154</td>\n",
       "      <td>18059</td>\n",
       "      <td>0.021211</td>\n",
       "      <td>1997</td>\n",
       "      <td>1996</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "      <td>13074</td>\n",
       "      <td>16171</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>1993</td>\n",
       "      <td>1995</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "      <td>27486</td>\n",
       "      <td>9702</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>1999</td>\n",
       "      <td>2003</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "      <td>19856</td>\n",
       "      <td>8212</td>\n",
       "      <td>0.059588</td>\n",
       "      <td>1997</td>\n",
       "      <td>2002</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2  link   rno1   rno2       sim  year1  year2  year_diff\n",
       "0  9510123  9502114     1  16827  15446  0.064373   1995   1995        0.0\n",
       "1  9707075  9604178     1  21154  18059  0.021211   1997   1996        1.0\n",
       "2  9312155  9506142     0  13074  16171  0.017202   1993   1995       -2.0\n",
       "3  9911255   302165     0  27486   9702  0.012634   1999   2003       -4.0\n",
       "4  9701033   209076     0  19856   8212  0.059588   1997   2002       -5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['year1'] = train.apply(lambda row: node_info.loc[node_info['id']==row['id1'], 'year'].values[0], axis=1)\n",
    "train['year2'] = train.apply(lambda row: node_info.loc[node_info['id']==row['id2'], 'year'].values[0], axis=1)\n",
    "train['year_diff'] = train.apply(lambda row: row['year1'] - row['year2'], axis=1)\n",
    "train['year_diff'] = train['year_diff'].astype(int)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>rno1</th>\n",
       "      <th>rno2</th>\n",
       "      <th>sim</th>\n",
       "      <th>year1</th>\n",
       "      <th>year2</th>\n",
       "      <th>diff_year</th>\n",
       "      <th>year_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9807076</td>\n",
       "      <td>9807139</td>\n",
       "      <td>23774</td>\n",
       "      <td>23835</td>\n",
       "      <td>0.071870</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109162</td>\n",
       "      <td>1182</td>\n",
       "      <td>5227</td>\n",
       "      <td>172</td>\n",
       "      <td>0.163040</td>\n",
       "      <td>2001</td>\n",
       "      <td>2000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9702187</td>\n",
       "      <td>9510135</td>\n",
       "      <td>20185</td>\n",
       "      <td>16838</td>\n",
       "      <td>0.138004</td>\n",
       "      <td>1997</td>\n",
       "      <td>1995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111048</td>\n",
       "      <td>110115</td>\n",
       "      <td>5621</td>\n",
       "      <td>5397</td>\n",
       "      <td>0.101857</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9910176</td>\n",
       "      <td>9410073</td>\n",
       "      <td>27159</td>\n",
       "      <td>14643</td>\n",
       "      <td>0.091231</td>\n",
       "      <td>1999</td>\n",
       "      <td>1994</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2   rno1   rno2       sim  year1  year2  diff_year  \\\n",
       "0  9807076  9807139  23774  23835  0.071870   1998   1998        0.0   \n",
       "1   109162     1182   5227    172  0.163040   2001   2000        1.0   \n",
       "2  9702187  9510135  20185  16838  0.138004   1997   1995        2.0   \n",
       "3   111048   110115   5621   5397  0.101857   2001   2001        0.0   \n",
       "4  9910176  9410073  27159  14643  0.091231   1999   1994        5.0   \n",
       "\n",
       "   year_diff  \n",
       "0          0  \n",
       "1          1  \n",
       "2          2  \n",
       "3          0  \n",
       "4          5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['year1'] = test.apply(lambda row: node_info.loc[node_info['id']==row['id1'], 'year'].values[0], axis=1)\n",
    "test['year2'] = test.apply(lambda row: node_info.loc[node_info['id']==row['id2'], 'year'].values[0], axis=1)\n",
    "test['year_diff'] = test.apply(lambda row: row['year1'] - row['year2'], axis=1)\n",
    "test['year_diff'] = test['year_diff'].astype(int)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAuthor(authors):\n",
    "    names = authors.split(',')\n",
    "    names = [name.replace('.', ' ').split() for name in names]\n",
    "    names = set([' '.join(name) for name in names])\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "      <td>{paul s aspinwall}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "      <td>{m cvetic, c n pope, h lu}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "      <td>{gungwon kang, y s myung}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "      <td>{adam d helfer}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "      <td>{c schweigert, j fuchs}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title  \\\n",
       "0  1001  2000              compactification geometry and duality   \n",
       "1  1002  2000  domain walls and massive gauged supergravity p...   \n",
       "2  1003  2000     comment on metric fluctuations in brane worlds   \n",
       "3  1004  2000         moving mirrors and thermodynamic paradoxes   \n",
       "4  1005  2000  bundles of chiral blocks and boundary conditio...   \n",
       "\n",
       "                       authors            journal  \\\n",
       "0            Paul S. Aspinwall                NaN   \n",
       "1  M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "2     Y.S. Myung, Gungwon Kang                NaN   \n",
       "3               Adam D. Helfer          Phys.Rev.   \n",
       "4      J. Fuchs, C. Schweigert                NaN   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  these are notes based on lectures given at tas...   \n",
       "1  we point out that massive gauged supergravity ...   \n",
       "2  recently ivanov and volovich hep-th 9912242 cl...   \n",
       "3  quantum fields responding to moving mirrors ha...   \n",
       "4  proceedings of lie iii clausthal july 1999 var...   \n",
       "\n",
       "                authors_split  \n",
       "0          {paul s aspinwall}  \n",
       "1  {m cvetic, c n pope, h lu}  \n",
       "2   {gungwon kang, y s myung}  \n",
       "3             {adam d helfer}  \n",
       "4     {c schweigert, j fuchs}  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_info['authors_split'] = node_info.apply(lambda row: processAuthor(row['authors'].lower()) \n",
    "                                             if isinstance(row['authors'], str) else set(), \n",
    "                                             axis=1)\n",
    "node_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCommonAuthors(id1, id2):\n",
    "    authors1 = node_info.loc[node_info['id']==id1, 'authors_split'].values[0]\n",
    "    authors2 = node_info.loc[node_info['id']==id2, 'authors_split'].values[0]\n",
    "    return len(authors1 & authors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>link</th>\n",
       "      <th>rno1</th>\n",
       "      <th>rno2</th>\n",
       "      <th>sim</th>\n",
       "      <th>year1</th>\n",
       "      <th>year2</th>\n",
       "      <th>year_diff</th>\n",
       "      <th>common_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "      <td>16827</td>\n",
       "      <td>15446</td>\n",
       "      <td>0.064373</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "      <td>21154</td>\n",
       "      <td>18059</td>\n",
       "      <td>0.021211</td>\n",
       "      <td>1997</td>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "      <td>13074</td>\n",
       "      <td>16171</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>1993</td>\n",
       "      <td>1995</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "      <td>27486</td>\n",
       "      <td>9702</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>1999</td>\n",
       "      <td>2003</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "      <td>19856</td>\n",
       "      <td>8212</td>\n",
       "      <td>0.059588</td>\n",
       "      <td>1997</td>\n",
       "      <td>2002</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2  link   rno1   rno2       sim  year1  year2  year_diff  \\\n",
       "0  9510123  9502114     1  16827  15446  0.064373   1995   1995          0   \n",
       "1  9707075  9604178     1  21154  18059  0.021211   1997   1996          1   \n",
       "2  9312155  9506142     0  13074  16171  0.017202   1993   1995         -2   \n",
       "3  9911255   302165     0  27486   9702  0.012634   1999   2003         -4   \n",
       "4  9701033   209076     0  19856   8212  0.059588   1997   2002         -5   \n",
       "\n",
       "   common_authors  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['common_authors'] = train.apply(lambda row: computeCommonAuthors(row['id1'], row['id2']), axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>rno1</th>\n",
       "      <th>rno2</th>\n",
       "      <th>sim</th>\n",
       "      <th>year1</th>\n",
       "      <th>year2</th>\n",
       "      <th>diff_year</th>\n",
       "      <th>year_diff</th>\n",
       "      <th>common_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9807076</td>\n",
       "      <td>9807139</td>\n",
       "      <td>23774</td>\n",
       "      <td>23835</td>\n",
       "      <td>0.071870</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109162</td>\n",
       "      <td>1182</td>\n",
       "      <td>5227</td>\n",
       "      <td>172</td>\n",
       "      <td>0.163040</td>\n",
       "      <td>2001</td>\n",
       "      <td>2000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9702187</td>\n",
       "      <td>9510135</td>\n",
       "      <td>20185</td>\n",
       "      <td>16838</td>\n",
       "      <td>0.138004</td>\n",
       "      <td>1997</td>\n",
       "      <td>1995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111048</td>\n",
       "      <td>110115</td>\n",
       "      <td>5621</td>\n",
       "      <td>5397</td>\n",
       "      <td>0.101857</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9910176</td>\n",
       "      <td>9410073</td>\n",
       "      <td>27159</td>\n",
       "      <td>14643</td>\n",
       "      <td>0.091231</td>\n",
       "      <td>1999</td>\n",
       "      <td>1994</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2   rno1   rno2       sim  year1  year2  diff_year  \\\n",
       "0  9807076  9807139  23774  23835  0.071870   1998   1998        0.0   \n",
       "1   109162     1182   5227    172  0.163040   2001   2000        1.0   \n",
       "2  9702187  9510135  20185  16838  0.138004   1997   1995        2.0   \n",
       "3   111048   110115   5621   5397  0.101857   2001   2001        0.0   \n",
       "4  9910176  9410073  27159  14643  0.091231   1999   1994        5.0   \n",
       "\n",
       "   year_diff  common_authors  \n",
       "0          0               0  \n",
       "1          1               0  \n",
       "2          2               0  \n",
       "3          0               0  \n",
       "4          5               0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['common_authors'] = test.apply(lambda row: computeCommonAuthors(row['id1'], row['id2']), axis=1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>link</th>\n",
       "      <th>pa</th>\n",
       "      <th>rai</th>\n",
       "      <th>aai</th>\n",
       "      <th>jc</th>\n",
       "      <th>pred_pa</th>\n",
       "      <th>pred_rai</th>\n",
       "      <th>pred_aai</th>\n",
       "      <th>pred_jc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.513898</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "      <td>11613.0</td>\n",
       "      <td>0.226401</td>\n",
       "      <td>4.320366</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2  link       pa       rai       aai        jc  pred_pa  \\\n",
       "0  9510123  9502114     1     72.0  0.142857  0.513898  0.058824        0   \n",
       "1  9707075  9604178     1  11613.0  0.226401  4.320366  0.097087        1   \n",
       "2  9312155  9506142     0      5.0  0.000000  0.000000  0.000000        0   \n",
       "3  9911255   302165     0    280.0  0.000000  0.000000  0.000000        0   \n",
       "4  9701033   209076     0    168.0  0.000000  0.000000  0.000000        0   \n",
       "\n",
       "   pred_rai  pred_aai  pred_jc  \n",
       "0         1         1        1  \n",
       "1         1         1        1  \n",
       "2         0         0        0  \n",
       "3         0         0        0  \n",
       "4         0         0        0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph = pd.read_csv('train_by_nx.csv', index_col=0)\n",
    "train_graph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>link</th>\n",
       "      <th>pa</th>\n",
       "      <th>rai</th>\n",
       "      <th>aai</th>\n",
       "      <th>jc</th>\n",
       "      <th>pred_pa</th>\n",
       "      <th>pred_rai</th>\n",
       "      <th>pred_aai</th>\n",
       "      <th>pred_jc</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.513898</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "      <td>11613.0</td>\n",
       "      <td>0.226401</td>\n",
       "      <td>4.320366</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2  link       pa       rai       aai        jc  pred_pa  \\\n",
       "0  9510123  9502114     1     72.0  0.142857  0.513898  0.058824        0   \n",
       "1  9707075  9604178     1  11613.0  0.226401  4.320366  0.097087        1   \n",
       "2  9312155  9506142     0      5.0  0.000000  0.000000  0.000000        0   \n",
       "3  9911255   302165     0    280.0  0.000000  0.000000  0.000000        0   \n",
       "4  9701033   209076     0    168.0  0.000000  0.000000  0.000000        0   \n",
       "\n",
       "   pred_rai  pred_aai  pred_jc  pred  \n",
       "0         1         1        1     1  \n",
       "1         1         1        1     1  \n",
       "2         0         0        0     0  \n",
       "3         0         0        0     0  \n",
       "4         0         0        0     0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph['pred'] = train_graph.apply(lambda row: 1 if (row['pred_rai']+row['pred_aai']+row['pred_jc'])>1 else 0, \n",
    "                                        axis=1)\n",
    "train_graph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>pa</th>\n",
       "      <th>rai</th>\n",
       "      <th>aai</th>\n",
       "      <th>jc</th>\n",
       "      <th>pred_pa</th>\n",
       "      <th>pred_rai</th>\n",
       "      <th>pred_aai</th>\n",
       "      <th>pred_jc</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9807076</td>\n",
       "      <td>9807139</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109162</td>\n",
       "      <td>1182</td>\n",
       "      <td>13590.0</td>\n",
       "      <td>0.311535</td>\n",
       "      <td>5.377973</td>\n",
       "      <td>0.074303</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9702187</td>\n",
       "      <td>9510135</td>\n",
       "      <td>164797.0</td>\n",
       "      <td>1.342594</td>\n",
       "      <td>15.053612</td>\n",
       "      <td>0.065338</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111048</td>\n",
       "      <td>110115</td>\n",
       "      <td>3315.0</td>\n",
       "      <td>0.298419</td>\n",
       "      <td>4.899424</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9910176</td>\n",
       "      <td>9410073</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2        pa       rai        aai        jc  pred_pa  \\\n",
       "0  9807076  9807139    1062.0  0.000000   0.000000  0.000000        1   \n",
       "1   109162     1182   13590.0  0.311535   5.377973  0.074303        1   \n",
       "2  9702187  9510135  164797.0  1.342594  15.053612  0.065338        1   \n",
       "3   111048   110115    3315.0  0.298419   4.899424  0.221053        1   \n",
       "4  9910176  9410073    1050.0  0.000000   0.000000  0.000000        1   \n",
       "\n",
       "   pred_rai  pred_aai  pred_jc  pred  \n",
       "0         0         0        0     0  \n",
       "1         1         1        1     1  \n",
       "2         1         1        1     1  \n",
       "3         1         1        1     1  \n",
       "4         0         0        0     0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph = pd.read_csv('test_by_nx.csv', index_col=0)\n",
    "test_graph['pred'] = test_graph.apply(lambda row: 1 if (row['pred_rai']+row['pred_aai']+row['pred_jc'])>1 else 0, \n",
    "                                        axis=1)\n",
    "test_graph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>link</th>\n",
       "      <th>rno1</th>\n",
       "      <th>rno2</th>\n",
       "      <th>sim</th>\n",
       "      <th>year1</th>\n",
       "      <th>year2</th>\n",
       "      <th>year_diff</th>\n",
       "      <th>common_authors</th>\n",
       "      <th>graph_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "      <td>16827</td>\n",
       "      <td>15446</td>\n",
       "      <td>0.064373</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "      <td>21154</td>\n",
       "      <td>18059</td>\n",
       "      <td>0.021211</td>\n",
       "      <td>1997</td>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "      <td>13074</td>\n",
       "      <td>16171</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>1993</td>\n",
       "      <td>1995</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "      <td>27486</td>\n",
       "      <td>9702</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>1999</td>\n",
       "      <td>2003</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "      <td>19856</td>\n",
       "      <td>8212</td>\n",
       "      <td>0.059588</td>\n",
       "      <td>1997</td>\n",
       "      <td>2002</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2  link   rno1   rno2       sim  year1  year2  year_diff  \\\n",
       "0  9510123  9502114     1  16827  15446  0.064373   1995   1995          0   \n",
       "1  9707075  9604178     1  21154  18059  0.021211   1997   1996          1   \n",
       "2  9312155  9506142     0  13074  16171  0.017202   1993   1995         -2   \n",
       "3  9911255   302165     0  27486   9702  0.012634   1999   2003         -4   \n",
       "4  9701033   209076     0  19856   8212  0.059588   1997   2002         -5   \n",
       "\n",
       "   common_authors  graph_pred  \n",
       "0               0           1  \n",
       "1               0           1  \n",
       "2               0           0  \n",
       "3               0           0  \n",
       "4               0           0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['graph_pred'] = train_graph['pred']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>rno1</th>\n",
       "      <th>rno2</th>\n",
       "      <th>sim</th>\n",
       "      <th>year1</th>\n",
       "      <th>year2</th>\n",
       "      <th>diff_year</th>\n",
       "      <th>year_diff</th>\n",
       "      <th>common_authors</th>\n",
       "      <th>graph_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9807076</td>\n",
       "      <td>9807139</td>\n",
       "      <td>23774</td>\n",
       "      <td>23835</td>\n",
       "      <td>0.071870</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109162</td>\n",
       "      <td>1182</td>\n",
       "      <td>5227</td>\n",
       "      <td>172</td>\n",
       "      <td>0.163040</td>\n",
       "      <td>2001</td>\n",
       "      <td>2000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9702187</td>\n",
       "      <td>9510135</td>\n",
       "      <td>20185</td>\n",
       "      <td>16838</td>\n",
       "      <td>0.138004</td>\n",
       "      <td>1997</td>\n",
       "      <td>1995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111048</td>\n",
       "      <td>110115</td>\n",
       "      <td>5621</td>\n",
       "      <td>5397</td>\n",
       "      <td>0.101857</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9910176</td>\n",
       "      <td>9410073</td>\n",
       "      <td>27159</td>\n",
       "      <td>14643</td>\n",
       "      <td>0.091231</td>\n",
       "      <td>1999</td>\n",
       "      <td>1994</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2   rno1   rno2       sim  year1  year2  diff_year  \\\n",
       "0  9807076  9807139  23774  23835  0.071870   1998   1998        0.0   \n",
       "1   109162     1182   5227    172  0.163040   2001   2000        1.0   \n",
       "2  9702187  9510135  20185  16838  0.138004   1997   1995        2.0   \n",
       "3   111048   110115   5621   5397  0.101857   2001   2001        0.0   \n",
       "4  9910176  9410073  27159  14643  0.091231   1999   1994        5.0   \n",
       "\n",
       "   year_diff  common_authors  graph_pred  \n",
       "0          0               0           0  \n",
       "1          1               0           1  \n",
       "2          2               0           1  \n",
       "3          0               0           1  \n",
       "4          5               0           0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['graph_pred'] = test_graph['pred']\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stpwds = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTitle(title):\n",
    "    if not isinstance(title, str):\n",
    "        return set()\n",
    "    t = title.split()\n",
    "    t = [token for token in t if token not in stpwds]\n",
    "    t = [stemmer.stem(token) for token in t]\n",
    "    return set(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors_split</th>\n",
       "      <th>title_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "      <td>{paul s aspinwall}</td>\n",
       "      <td>{geometri, dualiti, compactif}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "      <td>{m cvetic, c n pope, h lu}</td>\n",
       "      <td>{potenti, gaug, massiv, domain, supergrav, wall}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>2000</td>\n",
       "      <td>comment on metric fluctuations in brane worlds</td>\n",
       "      <td>Y.S. Myung, Gungwon Kang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n",
       "      <td>{gungwon kang, y s myung}</td>\n",
       "      <td>{comment, metric, brane, world, fluctuat}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>2000</td>\n",
       "      <td>moving mirrors and thermodynamic paradoxes</td>\n",
       "      <td>Adam D. Helfer</td>\n",
       "      <td>Phys.Rev.</td>\n",
       "      <td>quantum fields responding to moving mirrors ha...</td>\n",
       "      <td>{adam d helfer}</td>\n",
       "      <td>{thermodynam, paradox, mirror, move}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>2000</td>\n",
       "      <td>bundles of chiral blocks and boundary conditio...</td>\n",
       "      <td>J. Fuchs, C. Schweigert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>proceedings of lie iii clausthal july 1999 var...</td>\n",
       "      <td>{c schweigert, j fuchs}</td>\n",
       "      <td>{block, cft, chiral, condit, bundl, boundari}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title  \\\n",
       "0  1001  2000              compactification geometry and duality   \n",
       "1  1002  2000  domain walls and massive gauged supergravity p...   \n",
       "2  1003  2000     comment on metric fluctuations in brane worlds   \n",
       "3  1004  2000         moving mirrors and thermodynamic paradoxes   \n",
       "4  1005  2000  bundles of chiral blocks and boundary conditio...   \n",
       "\n",
       "                       authors            journal  \\\n",
       "0            Paul S. Aspinwall                NaN   \n",
       "1  M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "2     Y.S. Myung, Gungwon Kang                NaN   \n",
       "3               Adam D. Helfer          Phys.Rev.   \n",
       "4      J. Fuchs, C. Schweigert                NaN   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  these are notes based on lectures given at tas...   \n",
       "1  we point out that massive gauged supergravity ...   \n",
       "2  recently ivanov and volovich hep-th 9912242 cl...   \n",
       "3  quantum fields responding to moving mirrors ha...   \n",
       "4  proceedings of lie iii clausthal july 1999 var...   \n",
       "\n",
       "                authors_split  \\\n",
       "0          {paul s aspinwall}   \n",
       "1  {m cvetic, c n pope, h lu}   \n",
       "2   {gungwon kang, y s myung}   \n",
       "3             {adam d helfer}   \n",
       "4     {c schweigert, j fuchs}   \n",
       "\n",
       "                                        title_split  \n",
       "0                    {geometri, dualiti, compactif}  \n",
       "1  {potenti, gaug, massiv, domain, supergrav, wall}  \n",
       "2         {comment, metric, brane, world, fluctuat}  \n",
       "3              {thermodynam, paradox, mirror, move}  \n",
       "4     {block, cft, chiral, condit, bundl, boundari}  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_info['title_split'] = node_info.apply(lambda row: processTitle(row['title']), axis=1)\n",
    "node_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeOverlapTitle(id1, id2):\n",
    "    title1 = node_info.loc[node_info['id']==id1, 'title_split'].values[0]\n",
    "    title2 = node_info.loc[node_info['id']==id2, 'title_split'].values[0]\n",
    "    return len(title1 & title2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>link</th>\n",
       "      <th>rno1</th>\n",
       "      <th>rno2</th>\n",
       "      <th>sim</th>\n",
       "      <th>year1</th>\n",
       "      <th>year2</th>\n",
       "      <th>year_diff</th>\n",
       "      <th>common_authors</th>\n",
       "      <th>graph_pred</th>\n",
       "      <th>title_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "      <td>16827</td>\n",
       "      <td>15446</td>\n",
       "      <td>0.064373</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "      <td>21154</td>\n",
       "      <td>18059</td>\n",
       "      <td>0.021211</td>\n",
       "      <td>1997</td>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>0</td>\n",
       "      <td>13074</td>\n",
       "      <td>16171</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>1993</td>\n",
       "      <td>1995</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0</td>\n",
       "      <td>27486</td>\n",
       "      <td>9702</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>1999</td>\n",
       "      <td>2003</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>0</td>\n",
       "      <td>19856</td>\n",
       "      <td>8212</td>\n",
       "      <td>0.059588</td>\n",
       "      <td>1997</td>\n",
       "      <td>2002</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2  link   rno1   rno2       sim  year1  year2  year_diff  \\\n",
       "0  9510123  9502114     1  16827  15446  0.064373   1995   1995          0   \n",
       "1  9707075  9604178     1  21154  18059  0.021211   1997   1996          1   \n",
       "2  9312155  9506142     0  13074  16171  0.017202   1993   1995         -2   \n",
       "3  9911255   302165     0  27486   9702  0.012634   1999   2003         -4   \n",
       "4  9701033   209076     0  19856   8212  0.059588   1997   2002         -5   \n",
       "\n",
       "   common_authors  graph_pred  title_overlap  \n",
       "0               0           1              2  \n",
       "1               0           1              1  \n",
       "2               0           0              0  \n",
       "3               0           0              0  \n",
       "4               0           0              0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title_overlap'] = train.apply(lambda row: computeOverlapTitle(row['id1'], row['id2']), axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>rno1</th>\n",
       "      <th>rno2</th>\n",
       "      <th>sim</th>\n",
       "      <th>year1</th>\n",
       "      <th>year2</th>\n",
       "      <th>year_diff</th>\n",
       "      <th>common_authors</th>\n",
       "      <th>graph_pred</th>\n",
       "      <th>title_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9807076</td>\n",
       "      <td>9807139</td>\n",
       "      <td>23774</td>\n",
       "      <td>23835</td>\n",
       "      <td>0.071870</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109162</td>\n",
       "      <td>1182</td>\n",
       "      <td>5227</td>\n",
       "      <td>172</td>\n",
       "      <td>0.163040</td>\n",
       "      <td>2001</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9702187</td>\n",
       "      <td>9510135</td>\n",
       "      <td>20185</td>\n",
       "      <td>16838</td>\n",
       "      <td>0.138004</td>\n",
       "      <td>1997</td>\n",
       "      <td>1995</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111048</td>\n",
       "      <td>110115</td>\n",
       "      <td>5621</td>\n",
       "      <td>5397</td>\n",
       "      <td>0.101857</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9910176</td>\n",
       "      <td>9410073</td>\n",
       "      <td>27159</td>\n",
       "      <td>14643</td>\n",
       "      <td>0.091231</td>\n",
       "      <td>1999</td>\n",
       "      <td>1994</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id1      id2   rno1   rno2       sim  year1  year2  year_diff  \\\n",
       "0  9807076  9807139  23774  23835  0.071870   1998   1998          0   \n",
       "1   109162     1182   5227    172  0.163040   2001   2000          1   \n",
       "2  9702187  9510135  20185  16838  0.138004   1997   1995          2   \n",
       "3   111048   110115   5621   5397  0.101857   2001   2001          0   \n",
       "4  9910176  9410073  27159  14643  0.091231   1999   1994          5   \n",
       "\n",
       "   common_authors  graph_pred  title_overlap  \n",
       "0               0           0              0  \n",
       "1               0           1              2  \n",
       "2               0           1              1  \n",
       "3               0           1              1  \n",
       "4               0           0              0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['title_overlap'] = test.apply(lambda row: computeOverlapTitle(row['id1'], row['id2']), axis=1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import  MLPClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615512, 5)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "X_train = train[['sim', 'year_diff', 'common_authors', 'title_overlap', 'graph_pred']].values\n",
    "y_train = train['link'].values\n",
    "X_test = test[['sim', 'year_diff', 'common_authors', 'title_overlap', 'graph_pred']].values\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.19391591\n",
      "Iteration 1, loss = 0.19269004\n",
      "Iteration 1, loss = 0.19202641\n",
      "Iteration 1, loss = 0.19324635\n",
      "Iteration 2, loss = 0.11595245\n",
      "Iteration 2, loss = 0.11657824\n",
      "Iteration 2, loss = 0.11601181\n",
      "Iteration 2, loss = 0.11678786\n",
      "Iteration 3, loss = 0.11327494\n",
      "Iteration 3, loss = 0.11395152\n",
      "Iteration 3, loss = 0.11327578\n",
      "Iteration 3, loss = 0.11381893\n",
      "Iteration 4, loss = 0.11213078\n",
      "Iteration 4, loss = 0.11271955\n",
      "Iteration 4, loss = 0.11219484\n",
      "Iteration 4, loss = 0.11254294\n",
      "Iteration 5, loss = 0.11111719\n",
      "Iteration 5, loss = 0.11165518\n",
      "Iteration 5, loss = 0.11119874\n",
      "Iteration 5, loss = 0.11160837\n",
      "Iteration 6, loss = 0.11114214\n",
      "Iteration 6, loss = 0.11063658\n",
      "Iteration 6, loss = 0.11060503\n",
      "Iteration 6, loss = 0.11126512\n",
      "Iteration 7, loss = 0.11085414\n",
      "Iteration 7, loss = 0.11024095\n",
      "Iteration 7, loss = 0.11027974\n",
      "Iteration 7, loss = 0.11095793\n",
      "Iteration 8, loss = 0.11063764\n",
      "Iteration 8, loss = 0.11000887\n",
      "Iteration 8, loss = 0.11068767\n",
      "Iteration 8, loss = 0.10994302\n",
      "Iteration 9, loss = 0.11044690\n",
      "Iteration 9, loss = 0.10979967\n",
      "Iteration 9, loss = 0.11048397\n",
      "Iteration 9, loss = 0.10975081\n",
      "Iteration 10, loss = 0.10961658\n",
      "Iteration 10, loss = 0.11025220\n",
      "Iteration 10, loss = 0.11030354\n",
      "Iteration 10, loss = 0.10955358\n",
      "Iteration 11, loss = 0.10942910\n",
      "Iteration 11, loss = 0.11001562\n",
      "Iteration 11, loss = 0.11020744\n",
      "Iteration 11, loss = 0.10945019\n",
      "Iteration 12, loss = 0.10989430\n",
      "Iteration 12, loss = 0.11001736\n",
      "Iteration 12, loss = 0.10925788\n",
      "Iteration 12, loss = 0.10927076\n",
      "Iteration 13, loss = 0.10977262\n",
      "Iteration 13, loss = 0.10991711\n",
      "Iteration 13, loss = 0.10915055\n",
      "Iteration 13, loss = 0.10911742\n",
      "Iteration 14, loss = 0.10960930\n",
      "Iteration 14, loss = 0.10979341\n",
      "Iteration 14, loss = 0.10901957\n",
      "Iteration 14, loss = 0.10902857\n",
      "Iteration 15, loss = 0.10954498\n",
      "Iteration 15, loss = 0.10958378\n",
      "Iteration 15, loss = 0.10889708\n",
      "Iteration 15, loss = 0.10884639\n",
      "Iteration 16, loss = 0.10939120\n",
      "Iteration 16, loss = 0.10945610\n",
      "Iteration 16, loss = 0.10879366\n",
      "Iteration 16, loss = 0.10880013\n",
      "Iteration 17, loss = 0.10928679\n",
      "Iteration 17, loss = 0.10947514\n",
      "Iteration 17, loss = 0.10867088\n",
      "Iteration 17, loss = 0.10875212\n",
      "Iteration 18, loss = 0.10925757\n",
      "Iteration 18, loss = 0.10932432\n",
      "Iteration 18, loss = 0.10862677\n",
      "Iteration 18, loss = 0.10859057\n",
      "Iteration 19, loss = 0.10913520\n",
      "Iteration 19, loss = 0.10926494\n",
      "Iteration 19, loss = 0.10854880\n",
      "Iteration 19, loss = 0.10855321\n",
      "Iteration 20, loss = 0.10912676\n",
      "Iteration 20, loss = 0.10920102\n",
      "Iteration 20, loss = 0.10850565\n",
      "Iteration 20, loss = 0.10846052\n",
      "Iteration 21, loss = 0.10902431\n",
      "Iteration 21, loss = 0.10915923\n",
      "Iteration 21, loss = 0.10845947\n",
      "Iteration 21, loss = 0.10846822\n",
      "Iteration 22, loss = 0.10897881\n",
      "Iteration 22, loss = 0.10911609\n",
      "Iteration 22, loss = 0.10841740\n",
      "Iteration 22, loss = 0.10840763\n",
      "Iteration 23, loss = 0.10899549\n",
      "Iteration 23, loss = 0.10910197\n",
      "Iteration 23, loss = 0.10843465\n",
      "Iteration 23, loss = 0.10836609\n",
      "Iteration 24, loss = 0.10894456\n",
      "Iteration 24, loss = 0.10905437\n",
      "Iteration 24, loss = 0.10834155\n",
      "Iteration 24, loss = 0.10834111\n",
      "Iteration 25, loss = 0.10892157\n",
      "Iteration 25, loss = 0.10900678\n",
      "Iteration 25, loss = 0.10834016\n",
      "Iteration 25, loss = 0.10830987\n",
      "Iteration 26, loss = 0.10889899\n",
      "Iteration 26, loss = 0.10895273\n",
      "Iteration 26, loss = 0.10831626\n",
      "Iteration 26, loss = 0.10828179\n",
      "Iteration 27, loss = 0.10890851\n",
      "Iteration 27, loss = 0.10896470\n",
      "Iteration 27, loss = 0.10830877\n",
      "Iteration 27, loss = 0.10830688\n",
      "Iteration 28, loss = 0.10883134\n",
      "Iteration 28, loss = 0.10897837\n",
      "Iteration 28, loss = 0.10828567\n",
      "Iteration 28, loss = 0.10825119\n",
      "Iteration 29, loss = 0.10884835\n",
      "Iteration 29, loss = 0.10890889\n",
      "Iteration 29, loss = 0.10828566\n",
      "Iteration 29, loss = 0.10822080\n",
      "Iteration 30, loss = 0.10881872\n",
      "Iteration 30, loss = 0.10896415\n",
      "Iteration 30, loss = 0.10832346\n",
      "Iteration 30, loss = 0.10828463\n",
      "Iteration 31, loss = 0.10885000\n",
      "Iteration 31, loss = 0.10889853\n",
      "Iteration 31, loss = 0.10824138\n",
      "Iteration 31, loss = 0.10823647\n",
      "Iteration 32, loss = 0.10881951\n",
      "Iteration 32, loss = 0.10885532\n",
      "Iteration 32, loss = 0.10829428\n",
      "Iteration 32, loss = 0.10820632\n",
      "Iteration 33, loss = 0.10882175\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.10884000\n",
      "Iteration 33, loss = 0.10828339\n",
      "Iteration 33, loss = 0.10815480\n",
      "Iteration 34, loss = 0.10885239\n",
      "Iteration 34, loss = 0.10825612\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25974836\n",
      "Iteration 34, loss = 0.10812329\n",
      "Iteration 35, loss = 0.10814088\n",
      "Iteration 35, loss = 0.10884725\n",
      "Iteration 2, loss = 0.13451936\n",
      "Iteration 36, loss = 0.10812154\n",
      "Iteration 36, loss = 0.10882738\n",
      "Iteration 3, loss = 0.12285192\n",
      "Iteration 37, loss = 0.10811299\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.10881193\n",
      "Iteration 4, loss = 0.11874404\n",
      "Iteration 5, loss = 0.11663777\n",
      "Iteration 38, loss = 0.10879100\n",
      "Iteration 6, loss = 0.11516838\n",
      "Iteration 39, loss = 0.10883770\n",
      "Iteration 7, loss = 0.11398010\n",
      "Iteration 40, loss = 0.10877490\n",
      "Iteration 8, loss = 0.11265620\n",
      "Iteration 41, loss = 0.10877443\n",
      "Iteration 9, loss = 0.11152702\n",
      "Iteration 42, loss = 0.10882128\n",
      "Iteration 10, loss = 0.11090428\n",
      "Iteration 43, loss = 0.10885079\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.11052442\n",
      "Iteration 12, loss = 0.11016571\n",
      "Iteration 13, loss = 0.10994690\n",
      "Iteration 14, loss = 0.10985388\n",
      "Iteration 15, loss = 0.10972061\n",
      "Iteration 16, loss = 0.10958727\n",
      "Iteration 17, loss = 0.10947735\n",
      "Iteration 18, loss = 0.10941798\n",
      "Iteration 19, loss = 0.10937402\n",
      "Iteration 20, loss = 0.10929789\n",
      "Iteration 21, loss = 0.10920700\n",
      "Iteration 22, loss = 0.10919190\n",
      "Iteration 23, loss = 0.10920080\n",
      "Iteration 24, loss = 0.10912119\n",
      "Iteration 25, loss = 0.10904018\n",
      "Iteration 26, loss = 0.10902366\n",
      "Iteration 27, loss = 0.10903123\n",
      "Iteration 28, loss = 0.10892315\n",
      "Iteration 29, loss = 0.10890784\n",
      "Iteration 30, loss = 0.10889884\n",
      "Iteration 31, loss = 0.10893264\n",
      "Iteration 32, loss = 0.10891072\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19372724\n",
      "Iteration 1, loss = 0.19295013\n",
      "Iteration 1, loss = 0.19202072\n",
      "Iteration 1, loss = 0.19322589\n",
      "Iteration 2, loss = 0.11662846\n",
      "Iteration 2, loss = 0.11695237\n",
      "Iteration 2, loss = 0.11597923\n",
      "Iteration 2, loss = 0.11682760\n",
      "Iteration 3, loss = 0.11396094\n",
      "Iteration 3, loss = 0.11408958\n",
      "Iteration 3, loss = 0.11329189\n",
      "Iteration 3, loss = 0.11385366\n",
      "Iteration 4, loss = 0.11288396\n",
      "Iteration 4, loss = 0.11277093\n",
      "Iteration 4, loss = 0.11221327\n",
      "Iteration 4, loss = 0.11258358\n",
      "Iteration 5, loss = 0.11197935\n",
      "Iteration 5, loss = 0.11174076\n",
      "Iteration 5, loss = 0.11122077\n",
      "Iteration 5, loss = 0.11164613\n",
      "Iteration 6, loss = 0.11146531\n",
      "Iteration 6, loss = 0.11116924\n",
      "Iteration 6, loss = 0.11062661\n",
      "Iteration 6, loss = 0.11130916\n",
      "Iteration 7, loss = 0.11089511\n",
      "Iteration 7, loss = 0.11087941\n",
      "Iteration 7, loss = 0.11030901\n",
      "Iteration 7, loss = 0.11103765\n",
      "Iteration 8, loss = 0.11066570\n",
      "Iteration 8, loss = 0.11051865\n",
      "Iteration 8, loss = 0.10999274\n",
      "Iteration 8, loss = 0.11075238\n",
      "Iteration 9, loss = 0.11047959\n",
      "Iteration 9, loss = 0.11026106\n",
      "Iteration 9, loss = 0.10980805\n",
      "Iteration 9, loss = 0.11053953\n",
      "Iteration 10, loss = 0.11029072\n",
      "Iteration 10, loss = 0.11007263\n",
      "Iteration 10, loss = 0.10960583\n",
      "Iteration 10, loss = 0.11035464\n",
      "Iteration 11, loss = 0.11005085\n",
      "Iteration 11, loss = 0.10990020\n",
      "Iteration 11, loss = 0.10950223\n",
      "Iteration 11, loss = 0.11025446\n",
      "Iteration 12, loss = 0.10992257\n",
      "Iteration 12, loss = 0.10974195\n",
      "Iteration 12, loss = 0.10932616\n",
      "Iteration 12, loss = 0.11006946\n",
      "Iteration 13, loss = 0.10965711\n",
      "Iteration 13, loss = 0.10980361\n",
      "Iteration 13, loss = 0.10916785\n",
      "Iteration 13, loss = 0.10997233\n",
      "Iteration 14, loss = 0.10954963\n",
      "Iteration 14, loss = 0.10964778\n",
      "Iteration 14, loss = 0.10908161\n",
      "Iteration 14, loss = 0.10984389\n",
      "Iteration 15, loss = 0.10941425\n",
      "Iteration 15, loss = 0.10889974\n",
      "Iteration 15, loss = 0.10958807\n",
      "Iteration 15, loss = 0.10963756\n",
      "Iteration 16, loss = 0.10929589\n",
      "Iteration 16, loss = 0.10885423\n",
      "Iteration 16, loss = 0.10943763\n",
      "Iteration 16, loss = 0.10950901\n",
      "Iteration 17, loss = 0.10915485\n",
      "Iteration 17, loss = 0.10880141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.10933600\n",
      "Iteration 17, loss = 0.10952755\n",
      "Iteration 18, loss = 0.10909430\n",
      "Iteration 18, loss = 0.10864653\n",
      "Iteration 18, loss = 0.10930928\n",
      "Iteration 18, loss = 0.10937808\n",
      "Iteration 19, loss = 0.10860826\n",
      "Iteration 19, loss = 0.10898014\n",
      "Iteration 19, loss = 0.10931799\n",
      "Iteration 19, loss = 0.10918698\n",
      "Iteration 20, loss = 0.10851931\n",
      "Iteration 20, loss = 0.10893276\n",
      "Iteration 20, loss = 0.10924963\n",
      "Iteration 20, loss = 0.10918090\n",
      "Iteration 21, loss = 0.10852642\n",
      "Iteration 21, loss = 0.10920660\n",
      "Iteration 21, loss = 0.10888049\n",
      "Iteration 21, loss = 0.10908087\n",
      "Iteration 22, loss = 0.10846483\n",
      "Iteration 22, loss = 0.10916543\n",
      "Iteration 22, loss = 0.10904059\n",
      "Iteration 22, loss = 0.10879614\n",
      "Iteration 23, loss = 0.10843003\n",
      "Iteration 23, loss = 0.10915656\n",
      "Iteration 23, loss = 0.10905290\n",
      "Iteration 23, loss = 0.10877192\n",
      "Iteration 24, loss = 0.10839872\n",
      "Iteration 24, loss = 0.10911092\n",
      "Iteration 24, loss = 0.10900156\n",
      "Iteration 24, loss = 0.10865408\n",
      "Iteration 25, loss = 0.10836244\n",
      "Iteration 25, loss = 0.10897459\n",
      "Iteration 25, loss = 0.10905855\n",
      "Iteration 25, loss = 0.10859738\n",
      "Iteration 26, loss = 0.10834153\n",
      "Iteration 26, loss = 0.10895110\n",
      "Iteration 26, loss = 0.10852102\n",
      "Iteration 26, loss = 0.10900673\n",
      "Iteration 27, loss = 0.10896464\n",
      "Iteration 27, loss = 0.10836414\n",
      "Iteration 27, loss = 0.10846920\n",
      "Iteration 27, loss = 0.10901594\n",
      "Iteration 28, loss = 0.10888387\n",
      "Iteration 28, loss = 0.10840936\n",
      "Iteration 28, loss = 0.10903196\n",
      "Iteration 28, loss = 0.10830968\n",
      "Iteration 29, loss = 0.10890140\n",
      "Iteration 29, loss = 0.10838693\n",
      "Iteration 29, loss = 0.10896166\n",
      "Iteration 29, loss = 0.10827943\n",
      "Iteration 30, loss = 0.10887804\n",
      "Iteration 30, loss = 0.10901786\n",
      "Iteration 30, loss = 0.10834570\n",
      "Iteration 30, loss = 0.10839571\n",
      "Iteration 31, loss = 0.10890264\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.10895252\n",
      "Iteration 31, loss = 0.10827209\n",
      "Iteration 31, loss = 0.10830628\n",
      "Iteration 1, loss = 0.21564934\n",
      "Iteration 32, loss = 0.10829242\n",
      "Iteration 32, loss = 0.10890713\n",
      "Iteration 32, loss = 0.10827673\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.12235482\n",
      "Iteration 33, loss = 0.10889471\n",
      "Iteration 33, loss = 0.10826049\n",
      "Iteration 34, loss = 0.10890754\n",
      "Iteration 3, loss = 0.11852044\n",
      "Iteration 34, loss = 0.10820784\n",
      "Iteration 35, loss = 0.10889803\n",
      "Iteration 4, loss = 0.11655487\n",
      "Iteration 35, loss = 0.10815542\n",
      "Iteration 36, loss = 0.10888441\n",
      "Iteration 5, loss = 0.11492107\n",
      "Iteration 36, loss = 0.10813695\n",
      "Iteration 6, loss = 0.11351985\n",
      "Iteration 37, loss = 0.10887027\n",
      "Iteration 7, loss = 0.11241003\n",
      "Iteration 37, loss = 0.10813215\n",
      "Iteration 38, loss = 0.10885209\n",
      "Iteration 8, loss = 0.11155720\n",
      "Iteration 38, loss = 0.10805683\n",
      "Iteration 39, loss = 0.10889744\n",
      "Iteration 9, loss = 0.11078172\n",
      "Iteration 39, loss = 0.10810789\n",
      "Iteration 40, loss = 0.10883251\n",
      "Iteration 10, loss = 0.11007235\n",
      "Iteration 11, loss = 0.10958037\n",
      "Iteration 40, loss = 0.10805870\n",
      "Iteration 41, loss = 0.10883056\n",
      "Iteration 42, loss = 0.10888051\n",
      "Iteration 12, loss = 0.10924991\n",
      "Iteration 41, loss = 0.10806640\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.10891085\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.10901329\n",
      "Iteration 14, loss = 0.10879767\n",
      "Iteration 15, loss = 0.10862551\n",
      "Iteration 16, loss = 0.10855138\n",
      "Iteration 17, loss = 0.10838041\n",
      "Iteration 18, loss = 0.10837142\n",
      "Iteration 19, loss = 0.10828412\n",
      "Iteration 20, loss = 0.10824617\n",
      "Iteration 21, loss = 0.10821948\n",
      "Iteration 22, loss = 0.10818895\n",
      "Iteration 23, loss = 0.10816196\n",
      "Iteration 24, loss = 0.10817197\n",
      "Iteration 25, loss = 0.10810464\n",
      "Iteration 26, loss = 0.10809021\n",
      "Iteration 27, loss = 0.10805638\n",
      "Iteration 28, loss = 0.10806395\n",
      "Iteration 29, loss = 0.10805041\n",
      "Iteration 30, loss = 0.10802320\n",
      "Iteration 31, loss = 0.10800863\n",
      "Iteration 32, loss = 0.10799183\n",
      "Iteration 33, loss = 0.10804265\n",
      "Iteration 34, loss = 0.10796051\n",
      "Iteration 35, loss = 0.10797886\n",
      "Iteration 36, loss = 0.10791153\n",
      "Iteration 37, loss = 0.10790780\n",
      "Iteration 38, loss = 0.10793937\n",
      "Iteration 39, loss = 0.10793055\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19385609\n",
      "Iteration 1, loss = 0.19306817\n",
      "Iteration 1, loss = 0.19333611\n",
      "Iteration 1, loss = 0.19221584\n",
      "Iteration 2, loss = 0.11683305\n",
      "Iteration 2, loss = 0.11714415\n",
      "Iteration 2, loss = 0.11704352\n",
      "Iteration 2, loss = 0.11710983\n",
      "Iteration 3, loss = 0.11421718\n",
      "Iteration 3, loss = 0.11430936\n",
      "Iteration 3, loss = 0.11411686\n",
      "Iteration 3, loss = 0.11427833\n",
      "Iteration 4, loss = 0.11306893\n",
      "Iteration 4, loss = 0.11286439\n",
      "Iteration 4, loss = 0.11316571\n",
      "Iteration 4, loss = 0.11323185\n",
      "Iteration 5, loss = 0.11207406\n",
      "Iteration 5, loss = 0.11227631\n",
      "Iteration 5, loss = 0.11193852\n",
      "Iteration 5, loss = 0.11241216\n",
      "Iteration 6, loss = 0.11160536\n",
      "Iteration 6, loss = 0.11174283\n",
      "Iteration 6, loss = 0.11151452\n",
      "Iteration 6, loss = 0.11177114\n",
      "Iteration 7, loss = 0.11136216\n",
      "Iteration 7, loss = 0.11121810\n",
      "Iteration 7, loss = 0.11120294\n",
      "Iteration 7, loss = 0.11132506\n",
      "Iteration 8, loss = 0.11112840\n",
      "Iteration 8, loss = 0.11098778\n",
      "Iteration 8, loss = 0.11086209\n",
      "Iteration 8, loss = 0.11082404\n",
      "Iteration 9, loss = 0.11090128\n",
      "Iteration 9, loss = 0.11078592\n",
      "Iteration 9, loss = 0.11057076\n",
      "Iteration 9, loss = 0.11058869\n",
      "Iteration 10, loss = 0.11070601\n",
      "Iteration 10, loss = 0.11060098\n",
      "Iteration 10, loss = 0.11039996\n",
      "Iteration 10, loss = 0.11037145\n",
      "Iteration 11, loss = 0.11062231\n",
      "Iteration 11, loss = 0.11024175\n",
      "Iteration 11, loss = 0.11040502\n",
      "Iteration 11, loss = 0.11028066\n",
      "Iteration 12, loss = 0.11044139\n",
      "Iteration 12, loss = 0.11009257\n",
      "Iteration 12, loss = 0.11029973\n",
      "Iteration 12, loss = 0.11010185\n",
      "Iteration 13, loss = 0.11035130\n",
      "Iteration 13, loss = 0.11001104\n",
      "Iteration 13, loss = 0.10994037\n",
      "Iteration 13, loss = 0.11019256\n",
      "Iteration 14, loss = 0.11022976\n",
      "Iteration 14, loss = 0.10991954\n",
      "Iteration 14, loss = 0.10983979\n",
      "Iteration 14, loss = 0.11004582\n",
      "Iteration 15, loss = 0.11003225\n",
      "Iteration 15, loss = 0.10980622\n",
      "Iteration 15, loss = 0.10999165\n",
      "Iteration 15, loss = 0.10964796\n",
      "Iteration 16, loss = 0.10990966\n",
      "Iteration 16, loss = 0.10968837\n",
      "Iteration 16, loss = 0.10985538\n",
      "Iteration 16, loss = 0.10958313\n",
      "Iteration 17, loss = 0.10993189\n",
      "Iteration 17, loss = 0.10956062\n",
      "Iteration 17, loss = 0.10976612\n",
      "Iteration 17, loss = 0.10950818\n",
      "Iteration 18, loss = 0.10978375\n",
      "Iteration 18, loss = 0.10949306\n",
      "Iteration 18, loss = 0.10974767\n",
      "Iteration 18, loss = 0.10932985\n",
      "Iteration 19, loss = 0.10972608\n",
      "Iteration 19, loss = 0.10937387\n",
      "Iteration 19, loss = 0.10963541\n",
      "Iteration 19, loss = 0.10931832\n",
      "Iteration 20, loss = 0.10967137\n",
      "Iteration 20, loss = 0.10931436\n",
      "Iteration 20, loss = 0.10963491\n",
      "Iteration 20, loss = 0.10921449\n",
      "Iteration 21, loss = 0.10962574\n",
      "Iteration 21, loss = 0.10926603\n",
      "Iteration 21, loss = 0.10954106\n",
      "Iteration 21, loss = 0.10923165\n",
      "Iteration 22, loss = 0.10959251\n",
      "Iteration 22, loss = 0.10918716\n",
      "Iteration 22, loss = 0.10917012\n",
      "Iteration 22, loss = 0.10950096\n",
      "Iteration 23, loss = 0.10916482\n",
      "Iteration 23, loss = 0.10958013\n",
      "Iteration 23, loss = 0.10913257\n",
      "Iteration 23, loss = 0.10952831\n",
      "Iteration 24, loss = 0.10906851\n",
      "Iteration 24, loss = 0.10953943\n",
      "Iteration 24, loss = 0.10911462\n",
      "Iteration 24, loss = 0.10947625\n",
      "Iteration 25, loss = 0.10905055\n",
      "Iteration 25, loss = 0.10949185\n",
      "Iteration 25, loss = 0.10908318\n",
      "Iteration 25, loss = 0.10945401\n",
      "Iteration 26, loss = 0.10900692\n",
      "Iteration 26, loss = 0.10944364\n",
      "Iteration 26, loss = 0.10903665\n",
      "Iteration 26, loss = 0.10943326\n",
      "Iteration 27, loss = 0.10898132\n",
      "Iteration 27, loss = 0.10945728\n",
      "Iteration 27, loss = 0.10905419\n",
      "Iteration 27, loss = 0.10944686\n",
      "Iteration 28, loss = 0.10947491\n",
      "Iteration 28, loss = 0.10892094\n",
      "Iteration 28, loss = 0.10899224\n",
      "Iteration 28, loss = 0.10937073\n",
      "Iteration 29, loss = 0.10940669\n",
      "Iteration 29, loss = 0.10888417\n",
      "Iteration 29, loss = 0.10892204\n",
      "Iteration 29, loss = 0.10939157\n",
      "Iteration 30, loss = 0.10946065\n",
      "Iteration 30, loss = 0.10887727\n",
      "Iteration 30, loss = 0.10902919\n",
      "Iteration 30, loss = 0.10936752\n",
      "Iteration 31, loss = 0.10876077\n",
      "Iteration 31, loss = 0.10940298\n",
      "Iteration 31, loss = 0.10895746\n",
      "Iteration 31, loss = 0.10939213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.10878855\n",
      "Iteration 32, loss = 0.10935916\n",
      "Iteration 32, loss = 0.10889340\n",
      "Iteration 1, loss = 0.21584907\n",
      "Iteration 33, loss = 0.10878411\n",
      "Iteration 33, loss = 0.10934593\n",
      "Iteration 2, loss = 0.12252946\n",
      "Iteration 33, loss = 0.10881504\n",
      "Iteration 34, loss = 0.10869407\n",
      "Iteration 34, loss = 0.10935611\n",
      "Iteration 3, loss = 0.11875404\n",
      "Iteration 34, loss = 0.10876538\n",
      "Iteration 35, loss = 0.10861560\n",
      "Iteration 35, loss = 0.10935239\n",
      "Iteration 4, loss = 0.11682196\n",
      "Iteration 35, loss = 0.10873880\n",
      "Iteration 5, loss = 0.11522979\n",
      "Iteration 36, loss = 0.10858282\n",
      "Iteration 36, loss = 0.10933713\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.10873129\n",
      "Iteration 6, loss = 0.11383804\n",
      "Iteration 37, loss = 0.10856835\n",
      "Iteration 37, loss = 0.10873535\n",
      "Iteration 7, loss = 0.11273239\n",
      "Iteration 38, loss = 0.10846640\n",
      "Iteration 39, loss = 0.10852052\n",
      "Iteration 38, loss = 0.10870581\n",
      "Iteration 8, loss = 0.11189454\n",
      "Iteration 9, loss = 0.11113931\n",
      "Iteration 40, loss = 0.10846139\n",
      "Iteration 39, loss = 0.10874010\n",
      "Iteration 10, loss = 0.11046939\n",
      "Iteration 40, loss = 0.10866330\n",
      "Iteration 41, loss = 0.10845246\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.10999158\n",
      "Iteration 41, loss = 0.10860388\n",
      "Iteration 12, loss = 0.10968947\n",
      "Iteration 42, loss = 0.10872551\n",
      "Iteration 13, loss = 0.10944333\n",
      "Iteration 43, loss = 0.10864315\n",
      "Iteration 14, loss = 0.10922589\n",
      "Iteration 44, loss = 0.10856495\n",
      "Iteration 15, loss = 0.10905851\n",
      "Iteration 45, loss = 0.10852633\n",
      "Iteration 16, loss = 0.10895611\n",
      "Iteration 46, loss = 0.10858289\n",
      "Iteration 17, loss = 0.10879915\n",
      "Iteration 47, loss = 0.10852606\n",
      "Iteration 18, loss = 0.10877751\n",
      "Iteration 48, loss = 0.10845047\n",
      "Iteration 19, loss = 0.10868426\n",
      "Iteration 49, loss = 0.10842927\n",
      "Iteration 20, loss = 0.10865192\n",
      "Iteration 50, loss = 0.10843814\n",
      "Iteration 21, loss = 0.10862067\n",
      "Iteration 51, loss = 0.10842755\n",
      "Iteration 22, loss = 0.10860061\n",
      "Iteration 52, loss = 0.10834258\n",
      "Iteration 23, loss = 0.10857068\n",
      "Iteration 53, loss = 0.10837685\n",
      "Iteration 24, loss = 0.10858446\n",
      "Iteration 54, loss = 0.10830807\n",
      "Iteration 25, loss = 0.10852312\n",
      "Iteration 55, loss = 0.10834638\n",
      "Iteration 26, loss = 0.10851482\n",
      "Iteration 56, loss = 0.10836722\n",
      "Iteration 27, loss = 0.10848587\n",
      "Iteration 57, loss = 0.10833009\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.10848792\n",
      "Iteration 29, loss = 0.10847968\n",
      "Iteration 30, loss = 0.10845363\n",
      "Iteration 31, loss = 0.10844552\n",
      "Iteration 32, loss = 0.10842099\n",
      "Iteration 33, loss = 0.10848009\n",
      "Iteration 34, loss = 0.10839271\n",
      "Iteration 35, loss = 0.10842134\n",
      "Iteration 36, loss = 0.10835100\n",
      "Iteration 37, loss = 0.10833020\n",
      "Iteration 38, loss = 0.10836569\n",
      "Iteration 39, loss = 0.10835962\n",
      "Iteration 40, loss = 0.10834111\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19504180\n",
      "Iteration 1, loss = 0.19412926\n",
      "Iteration 1, loss = 0.19324782\n",
      "Iteration 1, loss = 0.19435432\n",
      "Iteration 2, loss = 0.11890539\n",
      "Iteration 2, loss = 0.11975745\n",
      "Iteration 2, loss = 0.11900958\n",
      "Iteration 2, loss = 0.11891907\n",
      "Iteration 3, loss = 0.11624275\n",
      "Iteration 3, loss = 0.11649653\n",
      "Iteration 3, loss = 0.11703963\n",
      "Iteration 3, loss = 0.11656419\n",
      "Iteration 4, loss = 0.11562868\n",
      "Iteration 4, loss = 0.11528308\n",
      "Iteration 4, loss = 0.11604386\n",
      "Iteration 4, loss = 0.11558984\n",
      "Iteration 5, loss = 0.11531314\n",
      "Iteration 5, loss = 0.11507094\n",
      "Iteration 5, loss = 0.11459044\n",
      "Iteration 5, loss = 0.11494963\n",
      "Iteration 6, loss = 0.11481467\n",
      "Iteration 6, loss = 0.11461053\n",
      "Iteration 6, loss = 0.11410139\n",
      "Iteration 6, loss = 0.11453568\n",
      "Iteration 7, loss = 0.11439902\n",
      "Iteration 7, loss = 0.11423421\n",
      "Iteration 7, loss = 0.11354904\n",
      "Iteration 7, loss = 0.11411022\n",
      "Iteration 8, loss = 0.11397843\n",
      "Iteration 8, loss = 0.11378996\n",
      "Iteration 8, loss = 0.11318150\n",
      "Iteration 8, loss = 0.11380477\n",
      "Iteration 9, loss = 0.11362980\n",
      "Iteration 9, loss = 0.11347448\n",
      "Iteration 9, loss = 0.11289288\n",
      "Iteration 9, loss = 0.11361491\n",
      "Iteration 10, loss = 0.11329635\n",
      "Iteration 10, loss = 0.11267246\n",
      "Iteration 10, loss = 0.11345911\n",
      "Iteration 10, loss = 0.11314775\n",
      "Iteration 11, loss = 0.11293876\n",
      "Iteration 11, loss = 0.11245047\n",
      "Iteration 11, loss = 0.11287373\n",
      "Iteration 11, loss = 0.11338220\n",
      "Iteration 12, loss = 0.11276226\n",
      "Iteration 12, loss = 0.11224022\n",
      "Iteration 12, loss = 0.11258130\n",
      "Iteration 12, loss = 0.11322707\n",
      "Iteration 13, loss = 0.11256736\n",
      "Iteration 13, loss = 0.11213970\n",
      "Iteration 13, loss = 0.11235517\n",
      "Iteration 13, loss = 0.11316860\n",
      "Iteration 14, loss = 0.11237433\n",
      "Iteration 14, loss = 0.11204094\n",
      "Iteration 14, loss = 0.11223975\n",
      "Iteration 14, loss = 0.11306423\n",
      "Iteration 15, loss = 0.11224696\n",
      "Iteration 15, loss = 0.11193255\n",
      "Iteration 15, loss = 0.11290915\n",
      "Iteration 16, loss = 0.11205562\n",
      "Iteration 15, loss = 0.11204121\n",
      "Iteration 16, loss = 0.11186300\n",
      "Iteration 17, loss = 0.11193698\n",
      "Iteration 16, loss = 0.11279427\n",
      "Iteration 16, loss = 0.11198337\n",
      "Iteration 17, loss = 0.11175921\n",
      "Iteration 18, loss = 0.11181682\n",
      "Iteration 17, loss = 0.11190362\n",
      "Iteration 18, loss = 0.11174855\n",
      "Iteration 17, loss = 0.11283306\n",
      "Iteration 19, loss = 0.11166564\n",
      "Iteration 18, loss = 0.11165105\n",
      "Iteration 19, loss = 0.11166627\n",
      "Iteration 18, loss = 0.11271857\n",
      "Iteration 20, loss = 0.11167989\n",
      "Iteration 19, loss = 0.11150656\n",
      "Iteration 20, loss = 0.11164707\n",
      "Iteration 21, loss = 0.11159108\n",
      "Iteration 19, loss = 0.11265857\n",
      "Iteration 20, loss = 0.11136389\n",
      "Iteration 21, loss = 0.11161882\n",
      "Iteration 22, loss = 0.11155975\n",
      "Iteration 20, loss = 0.11262765\n",
      "Iteration 21, loss = 0.11136106\n",
      "Iteration 23, loss = 0.11158287\n",
      "Iteration 22, loss = 0.11154896\n",
      "Iteration 21, loss = 0.11257145\n",
      "Iteration 24, loss = 0.11152601\n",
      "Iteration 22, loss = 0.11131420\n",
      "Iteration 23, loss = 0.11151185\n",
      "Iteration 22, loss = 0.11251855\n",
      "Iteration 25, loss = 0.11150393\n",
      "Iteration 24, loss = 0.11137772\n",
      "Iteration 23, loss = 0.11127185\n",
      "Iteration 23, loss = 0.11252105\n",
      "Iteration 26, loss = 0.11147928\n",
      "Iteration 25, loss = 0.11126637\n",
      "Iteration 24, loss = 0.11121243\n",
      "Iteration 24, loss = 0.11245018\n",
      "Iteration 27, loss = 0.11153006\n",
      "Iteration 26, loss = 0.11111635\n",
      "Iteration 25, loss = 0.11117101\n",
      "Iteration 25, loss = 0.11234426\n",
      "Iteration 28, loss = 0.11144041\n",
      "Iteration 27, loss = 0.11104801\n",
      "Iteration 26, loss = 0.11110224\n",
      "Iteration 26, loss = 0.11224745\n",
      "Iteration 29, loss = 0.11144661\n",
      "Iteration 28, loss = 0.11099352\n",
      "Iteration 27, loss = 0.11111741\n",
      "Iteration 30, loss = 0.11143885\n",
      "Iteration 27, loss = 0.11222274\n",
      "Iteration 29, loss = 0.11096536\n",
      "Iteration 28, loss = 0.11106688\n",
      "Iteration 28, loss = 0.11221899\n",
      "Iteration 31, loss = 0.11143879\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.11096710\n",
      "Iteration 1, loss = 0.21683718\n",
      "Iteration 29, loss = 0.11212790\n",
      "Iteration 29, loss = 0.11101983\n",
      "Iteration 31, loss = 0.11088980\n",
      "Iteration 2, loss = 0.12426036\n",
      "Iteration 30, loss = 0.11216623\n",
      "Iteration 32, loss = 0.11093236\n",
      "Iteration 30, loss = 0.11111891\n",
      "Iteration 3, loss = 0.12079724\n",
      "Iteration 33, loss = 0.11092111\n",
      "Iteration 31, loss = 0.11210168\n",
      "Iteration 31, loss = 0.11105588\n",
      "Iteration 4, loss = 0.11892707\n",
      "Iteration 34, loss = 0.11087451\n",
      "Iteration 5, loss = 0.11730510\n",
      "Iteration 32, loss = 0.11206924\n",
      "Iteration 32, loss = 0.11103355\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.11593188\n",
      "Iteration 35, loss = 0.11083018\n",
      "Iteration 33, loss = 0.11204285\n",
      "Iteration 7, loss = 0.11490192\n",
      "Iteration 36, loss = 0.11081069\n",
      "Iteration 34, loss = 0.11205026\n",
      "Iteration 8, loss = 0.11414944\n",
      "Iteration 37, loss = 0.11080639\n",
      "Iteration 35, loss = 0.11202733\n",
      "Iteration 9, loss = 0.11352916\n",
      "Iteration 38, loss = 0.11071807\n",
      "Iteration 36, loss = 0.11203278\n",
      "Iteration 39, loss = 0.11076369\n",
      "Iteration 37, loss = 0.11200978\n",
      "Iteration 10, loss = 0.11303312\n",
      "Iteration 40, loss = 0.11070233\n",
      "Iteration 38, loss = 0.11199082\n",
      "Iteration 11, loss = 0.11265516\n",
      "Iteration 41, loss = 0.11070097\n",
      "Iteration 12, loss = 0.11243006\n",
      "Iteration 39, loss = 0.11203065\n",
      "Iteration 42, loss = 0.11067667\n",
      "Iteration 40, loss = 0.11197125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.11225963\n",
      "Iteration 43, loss = 0.11065571\n",
      "Iteration 41, loss = 0.11195953\n",
      "Iteration 14, loss = 0.11211450\n",
      "Iteration 44, loss = 0.11063998\n",
      "Iteration 42, loss = 0.11202384\n",
      "Iteration 45, loss = 0.11073151\n",
      "Iteration 15, loss = 0.11199788\n",
      "Iteration 46, loss = 0.11068884\n",
      "Iteration 16, loss = 0.11197722\n",
      "Iteration 43, loss = 0.11203984\n",
      "Iteration 47, loss = 0.11062394\n",
      "Iteration 17, loss = 0.11183486\n",
      "Iteration 44, loss = 0.11196767\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.11063194\n",
      "Iteration 18, loss = 0.11187985\n",
      "Iteration 49, loss = 0.11070052\n",
      "Iteration 19, loss = 0.11177708\n",
      "Iteration 50, loss = 0.11072480\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.11174690\n",
      "Iteration 21, loss = 0.11172266\n",
      "Iteration 22, loss = 0.11169118\n",
      "Iteration 23, loss = 0.11166146\n",
      "Iteration 24, loss = 0.11167208\n",
      "Iteration 25, loss = 0.11162518\n",
      "Iteration 26, loss = 0.11158539\n",
      "Iteration 27, loss = 0.11155418\n",
      "Iteration 28, loss = 0.11154704\n",
      "Iteration 29, loss = 0.11155004\n",
      "Iteration 30, loss = 0.11153572\n",
      "Iteration 31, loss = 0.11152720\n",
      "Iteration 32, loss = 0.11149126\n",
      "Iteration 33, loss = 0.11153442\n",
      "Iteration 34, loss = 0.11145410\n",
      "Iteration 35, loss = 0.11150138\n",
      "Iteration 36, loss = 0.11141887\n",
      "Iteration 37, loss = 0.11139639\n",
      "Iteration 38, loss = 0.11143384\n",
      "Iteration 39, loss = 0.11141462\n",
      "Iteration 40, loss = 0.11142191\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20791756\n",
      "Iteration 1, loss = 0.20687349\n",
      "Iteration 1, loss = 0.20606953\n",
      "Iteration 1, loss = 0.20716349\n",
      "Iteration 2, loss = 0.13312869\n",
      "Iteration 2, loss = 0.13263233\n",
      "Iteration 2, loss = 0.13241474\n",
      "Iteration 2, loss = 0.13333188\n",
      "Iteration 3, loss = 0.13105151\n",
      "Iteration 3, loss = 0.13031990\n",
      "Iteration 3, loss = 0.13055425\n",
      "Iteration 3, loss = 0.13117704\n",
      "Iteration 4, loss = 0.13018997\n",
      "Iteration 4, loss = 0.12945084\n",
      "Iteration 4, loss = 0.12971803\n",
      "Iteration 4, loss = 0.13027176\n",
      "Iteration 5, loss = 0.12954316\n",
      "Iteration 5, loss = 0.12886711\n",
      "Iteration 5, loss = 0.12910849\n",
      "Iteration 5, loss = 0.12962508\n",
      "Iteration 6, loss = 0.12907494\n",
      "Iteration 6, loss = 0.12840563\n",
      "Iteration 6, loss = 0.12868861\n",
      "Iteration 6, loss = 0.12923736\n",
      "Iteration 7, loss = 0.12875279\n",
      "Iteration 7, loss = 0.12806332\n",
      "Iteration 7, loss = 0.12825681\n",
      "Iteration 7, loss = 0.12879449\n",
      "Iteration 8, loss = 0.12843846\n",
      "Iteration 8, loss = 0.12771241\n",
      "Iteration 8, loss = 0.12793899\n",
      "Iteration 8, loss = 0.12843346\n",
      "Iteration 9, loss = 0.12820875\n",
      "Iteration 9, loss = 0.12747522\n",
      "Iteration 9, loss = 0.12766350\n",
      "Iteration 9, loss = 0.12813494\n",
      "Iteration 10, loss = 0.12795481\n",
      "Iteration 10, loss = 0.12722226\n",
      "Iteration 10, loss = 0.12741784\n",
      "Iteration 10, loss = 0.12785734\n",
      "Iteration 11, loss = 0.12770604\n",
      "Iteration 11, loss = 0.12705648\n",
      "Iteration 11, loss = 0.12719031\n",
      "Iteration 11, loss = 0.12767413\n",
      "Iteration 12, loss = 0.12751822\n",
      "Iteration 12, loss = 0.12683102\n",
      "Iteration 12, loss = 0.12696789\n",
      "Iteration 12, loss = 0.12741356\n",
      "Iteration 13, loss = 0.12730464\n",
      "Iteration 13, loss = 0.12679001\n",
      "Iteration 13, loss = 0.12663925\n",
      "Iteration 13, loss = 0.12726669\n",
      "Iteration 14, loss = 0.12711027\n",
      "Iteration 14, loss = 0.12664084\n",
      "Iteration 14, loss = 0.12647771\n",
      "Iteration 14, loss = 0.12705395\n",
      "Iteration 15, loss = 0.12696208\n",
      "Iteration 15, loss = 0.12645723\n",
      "Iteration 15, loss = 0.12628813\n",
      "Iteration 15, loss = 0.12685744\n",
      "Iteration 16, loss = 0.12674901\n",
      "Iteration 16, loss = 0.12630476\n",
      "Iteration 16, loss = 0.12614996\n",
      "Iteration 16, loss = 0.12665065\n",
      "Iteration 17, loss = 0.12612162\n",
      "Iteration 17, loss = 0.12659643\n",
      "Iteration 17, loss = 0.12659816\n",
      "Iteration 17, loss = 0.12606738\n",
      "Iteration 18, loss = 0.12605187\n",
      "Iteration 18, loss = 0.12649670\n",
      "Iteration 18, loss = 0.12644874\n",
      "Iteration 18, loss = 0.12587868\n",
      "Iteration 19, loss = 0.12636096\n",
      "Iteration 19, loss = 0.12591167\n",
      "Iteration 19, loss = 0.12633856\n",
      "Iteration 19, loss = 0.12580609\n",
      "Iteration 20, loss = 0.12627905\n",
      "Iteration 20, loss = 0.12580150\n",
      "Iteration 20, loss = 0.12625081\n",
      "Iteration 21, loss = 0.12617231\n",
      "Iteration 20, loss = 0.12568743\n",
      "Iteration 21, loss = 0.12576650\n",
      "Iteration 21, loss = 0.12615571\n",
      "Iteration 22, loss = 0.12607979\n",
      "Iteration 21, loss = 0.12562233\n",
      "Iteration 22, loss = 0.12564633\n",
      "Iteration 22, loss = 0.12609648\n",
      "Iteration 23, loss = 0.12606869\n",
      "Iteration 22, loss = 0.12554821\n",
      "Iteration 23, loss = 0.12561641\n",
      "Iteration 23, loss = 0.12610173\n",
      "Iteration 24, loss = 0.12602953\n",
      "Iteration 23, loss = 0.12550223\n",
      "Iteration 24, loss = 0.12555901\n",
      "Iteration 24, loss = 0.12604806\n",
      "Iteration 25, loss = 0.12599264\n",
      "Iteration 24, loss = 0.12546303\n",
      "Iteration 25, loss = 0.12553680\n",
      "Iteration 25, loss = 0.12601281\n",
      "Iteration 26, loss = 0.12596651\n",
      "Iteration 25, loss = 0.12543343\n",
      "Iteration 26, loss = 0.12551294\n",
      "Iteration 26, loss = 0.12591847\n",
      "Iteration 27, loss = 0.12599903\n",
      "Iteration 26, loss = 0.12537975\n",
      "Iteration 27, loss = 0.12548131\n",
      "Iteration 27, loss = 0.12588572\n",
      "Iteration 28, loss = 0.12593272\n",
      "Iteration 27, loss = 0.12537914\n",
      "Iteration 28, loss = 0.12542314\n",
      "Iteration 28, loss = 0.12589103\n",
      "Iteration 29, loss = 0.12591442\n",
      "Iteration 28, loss = 0.12528934\n",
      "Iteration 29, loss = 0.12540950\n",
      "Iteration 29, loss = 0.12582916\n",
      "Iteration 30, loss = 0.12588742\n",
      "Iteration 29, loss = 0.12523852\n",
      "Iteration 30, loss = 0.12539928\n",
      "Iteration 30, loss = 0.12588077\n",
      "Iteration 31, loss = 0.12589771\n",
      "Iteration 30, loss = 0.12532200\n",
      "Iteration 31, loss = 0.12533328\n",
      "Iteration 31, loss = 0.12580439\n",
      "Iteration 32, loss = 0.12588465\n",
      "Iteration 32, loss = 0.12535856\n",
      "Iteration 31, loss = 0.12525646\n",
      "Iteration 33, loss = 0.12582796\n",
      "Iteration 32, loss = 0.12577145\n",
      "Iteration 33, loss = 0.12535044\n",
      "Iteration 32, loss = 0.12523180\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.12580093\n",
      "Iteration 33, loss = 0.12575031\n",
      "Iteration 34, loss = 0.12531617\n",
      "Iteration 34, loss = 0.12574072\n",
      "Iteration 1, loss = 0.21530540\n",
      "Iteration 35, loss = 0.12575547\n",
      "Iteration 35, loss = 0.12529845\n",
      "Iteration 35, loss = 0.12572414\n",
      "Iteration 36, loss = 0.12576724\n",
      "Iteration 2, loss = 0.13241155\n",
      "Iteration 36, loss = 0.12528228\n",
      "Iteration 36, loss = 0.12572362\n",
      "Iteration 3, loss = 0.13007589\n",
      "Iteration 37, loss = 0.12574600\n",
      "Iteration 37, loss = 0.12526075\n",
      "Iteration 37, loss = 0.12572105\n",
      "Iteration 4, loss = 0.12905016\n",
      "Iteration 38, loss = 0.12574174\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.12524626\n",
      "Iteration 38, loss = 0.12568484\n",
      "Iteration 5, loss = 0.12852283\n",
      "Iteration 39, loss = 0.12572800\n",
      "Iteration 39, loss = 0.12526015\n",
      "Iteration 6, loss = 0.12815018\n",
      "Iteration 40, loss = 0.12567893\n",
      "Iteration 40, loss = 0.12520956\n",
      "Iteration 7, loss = 0.12778071\n",
      "Iteration 41, loss = 0.12566760\n",
      "Iteration 41, loss = 0.12522465\n",
      "Iteration 8, loss = 0.12751310\n",
      "Iteration 42, loss = 0.12570019\n",
      "Iteration 42, loss = 0.12521112\n",
      "Iteration 9, loss = 0.12731674\n",
      "Iteration 43, loss = 0.12573599\n",
      "Iteration 43, loss = 0.12520120\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.12707766\n",
      "Iteration 44, loss = 0.12566458\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.12690492\n",
      "Iteration 12, loss = 0.12683330\n",
      "Iteration 13, loss = 0.12676863\n",
      "Iteration 14, loss = 0.12672521\n",
      "Iteration 15, loss = 0.12664309\n",
      "Iteration 16, loss = 0.12657325\n",
      "Iteration 17, loss = 0.12658996\n",
      "Iteration 18, loss = 0.12655112\n",
      "Iteration 19, loss = 0.12655512\n",
      "Iteration 20, loss = 0.12648234\n",
      "Iteration 21, loss = 0.12650351\n",
      "Iteration 22, loss = 0.12645780\n",
      "Iteration 23, loss = 0.12644505\n",
      "Iteration 24, loss = 0.12640782\n",
      "Iteration 25, loss = 0.12640751\n",
      "Iteration 26, loss = 0.12637644\n",
      "Iteration 27, loss = 0.12637668\n",
      "Iteration 28, loss = 0.12634919\n",
      "Iteration 29, loss = 0.12634796\n",
      "Iteration 30, loss = 0.12632875\n",
      "Iteration 31, loss = 0.12629754\n",
      "Iteration 32, loss = 0.12627141\n",
      "Iteration 33, loss = 0.12628403\n",
      "Iteration 34, loss = 0.12626628\n",
      "Iteration 35, loss = 0.12622605\n",
      "Iteration 36, loss = 0.12625887\n",
      "Iteration 37, loss = 0.12625271\n",
      "Iteration 38, loss = 0.12621824\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.26965044\n",
      "Iteration 1, loss = 0.27053940\n",
      "Iteration 1, loss = 0.26860048\n",
      "Iteration 1, loss = 0.26952865\n",
      "Iteration 2, loss = 0.20355169\n",
      "Iteration 2, loss = 0.20388934\n",
      "Iteration 2, loss = 0.20327800\n",
      "Iteration 2, loss = 0.20394734\n",
      "Iteration 3, loss = 0.19953557\n",
      "Iteration 3, loss = 0.19993520\n",
      "Iteration 3, loss = 0.19997993\n",
      "Iteration 3, loss = 0.19933750\n",
      "Iteration 4, loss = 0.19847983\n",
      "Iteration 4, loss = 0.19886879\n",
      "Iteration 4, loss = 0.19826909\n",
      "Iteration 4, loss = 0.19893304\n",
      "Iteration 5, loss = 0.19810349\n",
      "Iteration 5, loss = 0.19848783\n",
      "Iteration 5, loss = 0.19788689\n",
      "Iteration 5, loss = 0.19855877\n",
      "Iteration 6, loss = 0.19798034\n",
      "Iteration 6, loss = 0.19832617\n",
      "Iteration 6, loss = 0.19776216\n",
      "Iteration 6, loss = 0.19845248\n",
      "Iteration 7, loss = 0.19789902\n",
      "Iteration 7, loss = 0.19826177\n",
      "Iteration 7, loss = 0.19768705\n",
      "Iteration 7, loss = 0.19831861\n",
      "Iteration 8, loss = 0.19782879\n",
      "Iteration 8, loss = 0.19820108\n",
      "Iteration 8, loss = 0.19757391\n",
      "Iteration 8, loss = 0.19824735\n",
      "Iteration 9, loss = 0.19779566\n",
      "Iteration 9, loss = 0.19819080\n",
      "Iteration 9, loss = 0.19758469\n",
      "Iteration 9, loss = 0.19822551\n",
      "Iteration 10, loss = 0.19774765\n",
      "Iteration 10, loss = 0.19815583\n",
      "Iteration 10, loss = 0.19821012\n",
      "Iteration 10, loss = 0.19754600\n",
      "Iteration 11, loss = 0.19778006\n",
      "Iteration 11, loss = 0.19813523\n",
      "Iteration 11, loss = 0.19819288\n",
      "Iteration 11, loss = 0.19753235\n",
      "Iteration 12, loss = 0.19811566\n",
      "Iteration 12, loss = 0.19773387\n",
      "Iteration 12, loss = 0.19817335\n",
      "Iteration 12, loss = 0.19753032\n",
      "Iteration 13, loss = 0.19772219\n",
      "Iteration 13, loss = 0.19811195\n",
      "Iteration 13, loss = 0.19748803\n",
      "Iteration 13, loss = 0.19819600\n",
      "Iteration 14, loss = 0.19774968\n",
      "Iteration 14, loss = 0.19810942\n",
      "Iteration 14, loss = 0.19751401\n",
      "Iteration 14, loss = 0.19818449\n",
      "Iteration 15, loss = 0.19773185\n",
      "Iteration 15, loss = 0.19811511\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.19747812\n",
      "Iteration 15, loss = 0.19814357\n",
      "Iteration 16, loss = 0.19770381\n",
      "Iteration 1, loss = 0.28896887\n",
      "Iteration 16, loss = 0.19746327\n",
      "Iteration 16, loss = 0.19810566\n",
      "Iteration 17, loss = 0.19768461\n",
      "Iteration 2, loss = 0.20337132\n",
      "Iteration 17, loss = 0.19749070\n",
      "Iteration 17, loss = 0.19813857\n",
      "Iteration 18, loss = 0.19769244\n",
      "Iteration 3, loss = 0.20155299\n",
      "Iteration 18, loss = 0.19745684\n",
      "Iteration 18, loss = 0.19814632\n",
      "Iteration 19, loss = 0.19769087\n",
      "Iteration 4, loss = 0.19981563\n",
      "Iteration 19, loss = 0.19747820\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.19812676\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.19766651\n",
      "Iteration 5, loss = 0.19841238\n",
      "Iteration 21, loss = 0.19770209\n",
      "Iteration 6, loss = 0.19804507\n",
      "Iteration 22, loss = 0.19765514\n",
      "Iteration 7, loss = 0.19787855\n",
      "Iteration 23, loss = 0.19765003\n",
      "Iteration 8, loss = 0.19771137\n",
      "Iteration 24, loss = 0.19764085\n",
      "Iteration 9, loss = 0.19769092\n",
      "Iteration 25, loss = 0.19764761\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.19754345\n",
      "Iteration 11, loss = 0.19752989\n",
      "Iteration 12, loss = 0.19747586\n",
      "Iteration 13, loss = 0.19746723\n",
      "Iteration 14, loss = 0.19749866\n",
      "Iteration 15, loss = 0.19744717\n",
      "Iteration 16, loss = 0.19741669\n",
      "Iteration 17, loss = 0.19737230\n",
      "Iteration 18, loss = 0.19738325\n",
      "Iteration 19, loss = 0.19738338\n",
      "Iteration 20, loss = 0.19739014\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9699970659715913,\n",
       " 0.96995801722945,\n",
       " 0.9700093382084922,\n",
       " 0.9698436089402058,\n",
       " 0.9693278486942362,\n",
       " 0.9667602550046942]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores = []\n",
    "for alpha in alphas:\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(10, 5), \n",
    "                        alpha=alpha, \n",
    "                        max_iter=500, \n",
    "                        tol=1e-5,\n",
    "                        verbose=True)\n",
    "    accuracy = cross_val_score(clf, \n",
    "                               X_train, \n",
    "                               y_train, \n",
    "                               cv=5, \n",
    "                               scoring='f1', \n",
    "                               n_jobs=-1).mean()\n",
    "    f1_scores.append(accuracy)\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7b9e0dbb70>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b9e91aa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.xscale('log')\n",
    "plt.plot(alphas, f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.17793169\n",
      "Iteration 2, loss = 0.11491154\n",
      "Iteration 3, loss = 0.11306293\n",
      "Iteration 4, loss = 0.11185990\n",
      "Iteration 5, loss = 0.11112431\n",
      "Iteration 6, loss = 0.11076023\n",
      "Iteration 7, loss = 0.11038785\n",
      "Iteration 8, loss = 0.11024552\n",
      "Iteration 9, loss = 0.11001581\n",
      "Iteration 10, loss = 0.10985000\n",
      "Iteration 11, loss = 0.10969251\n",
      "Iteration 12, loss = 0.10955845\n",
      "Iteration 13, loss = 0.10945984\n",
      "Iteration 14, loss = 0.10935913\n",
      "Iteration 15, loss = 0.10929808\n",
      "Iteration 16, loss = 0.10928825\n",
      "Iteration 17, loss = 0.10918117\n",
      "Iteration 18, loss = 0.10918009\n",
      "Iteration 19, loss = 0.10919075\n",
      "Iteration 20, loss = 0.10909128\n",
      "Iteration 21, loss = 0.10909915\n",
      "Iteration 22, loss = 0.10907556\n",
      "Iteration 23, loss = 0.10902277\n",
      "Iteration 24, loss = 0.10902000\n",
      "Iteration 25, loss = 0.10904758\n",
      "Iteration 26, loss = 0.10898475\n",
      "Iteration 27, loss = 0.10894501\n",
      "Iteration 28, loss = 0.10896081\n",
      "Iteration 29, loss = 0.10893795\n",
      "Iteration 30, loss = 0.10896497\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 5), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=1e-05, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(10, 5), \n",
    "                    alpha=1e-3, \n",
    "                    max_iter=500, \n",
    "                    tol=1e-5,\n",
    "                    verbose=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        0\n",
       "5        0\n",
       "6        0\n",
       "7        1\n",
       "8        0\n",
       "9        1\n",
       "10       1\n",
       "11       1\n",
       "12       1\n",
       "13       1\n",
       "14       1\n",
       "15       0\n",
       "16       0\n",
       "17       1\n",
       "18       1\n",
       "19       1\n",
       "20       1\n",
       "21       0\n",
       "22       1\n",
       "23       1\n",
       "24       0\n",
       "25       0\n",
       "26       1\n",
       "27       1\n",
       "28       0\n",
       "29       0\n",
       "        ..\n",
       "32618    0\n",
       "32619    1\n",
       "32620    1\n",
       "32621    0\n",
       "32622    1\n",
       "32623    0\n",
       "32624    0\n",
       "32625    0\n",
       "32626    1\n",
       "32627    0\n",
       "32628    1\n",
       "32629    0\n",
       "32630    0\n",
       "32631    1\n",
       "32632    1\n",
       "32633    0\n",
       "32634    0\n",
       "32635    1\n",
       "32636    1\n",
       "32637    0\n",
       "32638    0\n",
       "32639    0\n",
       "32640    0\n",
       "32641    0\n",
       "32642    1\n",
       "32643    1\n",
       "32644    0\n",
       "32645    0\n",
       "32646    0\n",
       "32647    1\n",
       "Length: 32648, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pd.Series(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32618</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32619</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32620</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32621</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32622</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32623</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32624</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32625</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32626</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32627</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32628</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32629</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32630</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32631</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32632</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32633</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32634</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32635</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32636</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32637</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32638</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32639</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32640</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32641</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32642</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32643</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32644</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32645</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32646</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32647</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32648 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category\n",
       "0             0\n",
       "1             1\n",
       "2             1\n",
       "3             1\n",
       "4             0\n",
       "5             0\n",
       "6             0\n",
       "7             1\n",
       "8             0\n",
       "9             1\n",
       "10            1\n",
       "11            1\n",
       "12            1\n",
       "13            1\n",
       "14            1\n",
       "15            0\n",
       "16            0\n",
       "17            1\n",
       "18            1\n",
       "19            1\n",
       "20            1\n",
       "21            0\n",
       "22            1\n",
       "23            1\n",
       "24            0\n",
       "25            0\n",
       "26            1\n",
       "27            1\n",
       "28            0\n",
       "29            0\n",
       "...         ...\n",
       "32618         0\n",
       "32619         1\n",
       "32620         1\n",
       "32621         0\n",
       "32622         1\n",
       "32623         0\n",
       "32624         0\n",
       "32625         0\n",
       "32626         1\n",
       "32627         0\n",
       "32628         1\n",
       "32629         0\n",
       "32630         0\n",
       "32631         1\n",
       "32632         1\n",
       "32633         0\n",
       "32634         0\n",
       "32635         1\n",
       "32636         1\n",
       "32637         0\n",
       "32638         0\n",
       "32639         0\n",
       "32640         0\n",
       "32641         0\n",
       "32642         1\n",
       "32643         1\n",
       "32644         0\n",
       "32645         0\n",
       "32646         0\n",
       "32647         1\n",
       "\n",
       "[32648 rows x 1 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_nn = pd.DataFrame()\n",
    "sub_nn['category'] = y_pred\n",
    "sub_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_nn.index.name='id'\n",
    "sub_nn.to_csv('sub_nn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category\n",
       "id          \n",
       "0          0\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_nn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
